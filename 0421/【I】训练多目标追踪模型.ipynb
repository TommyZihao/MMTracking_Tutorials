{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee09ba2-d6c3-44d1-adfa-2299d3702d7f",
   "metadata": {},
   "source": [
    "# 训练多目标追踪 MOT 模型\n",
    "\n",
    "如果报错`CUDA out of memory.`则重启前面几个代码的`kernel`即可。\n",
    "\n",
    "作者：同济子豪兄 2022-4-21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84dccb-940a-4115-980b-2f6d50fcaed3",
   "metadata": {},
   "source": [
    "## 进入 MMTracking 主目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad8307f3-4150-48bb-96d2-b2836d7499ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.circleci',\n",
       " '.dev_scripts',\n",
       " '.github',\n",
       " '.gitignore',\n",
       " '.pre-commit-config.yaml',\n",
       " '.readthedocs.yml',\n",
       " 'CITATION.cff',\n",
       " 'LICENSE',\n",
       " 'MANIFEST.in',\n",
       " 'README.md',\n",
       " 'README_zh-CN.md',\n",
       " 'configs',\n",
       " 'demo',\n",
       " 'docker',\n",
       " 'docs',\n",
       " 'mmtrack',\n",
       " 'model-index.yml',\n",
       " 'requirements.txt',\n",
       " 'requirements',\n",
       " 'resources',\n",
       " 'setup.cfg',\n",
       " 'setup.py',\n",
       " 'tests',\n",
       " 'tools',\n",
       " 'mmtrack.egg-info',\n",
       " 'checkpoints',\n",
       " 'outputs',\n",
       " 'data',\n",
       " '20220421145729',\n",
       " '20220421145729-plot',\n",
       " '20220421150001',\n",
       " '20220421150001-plot',\n",
       " '20220421150050',\n",
       " '20220421150050-plot',\n",
       " 'test.jpg',\n",
       " 'tutorial_exps',\n",
       " 'bytetrack_zihao']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('mmtracking')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8cf8e-54fb-431d-8f1c-bb496bb0c4e3",
   "metadata": {},
   "source": [
    "## 下载 MOT17 多目标追踪数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad21a574-2602-4fa3-a46b-b4c3bed15c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-21 18:41:01--  https://download.openmmlab.com/mmtracking/data/MOT17_tiny.zip\n",
      "Connecting to 172.16.0.13:5848... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 344566302 (329M) [application/zip]\n",
      "Saving to: ‘./data/MOT17_tiny.zip’\n",
      "\n",
      "MOT17_tiny.zip      100%[===================>] 328.60M  88.6MB/s    in 3.7s    \n",
      "\n",
      "2022-04-21 18:41:05 (88.3 MB/s) - ‘./data/MOT17_tiny.zip’ saved [344566302/344566302]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 下载 MOT17 数据集压缩包\n",
    "!wget https://download.openmmlab.com/mmtracking/data/MOT17_tiny.zip -P ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4cd48d-acd0-4d14-a6b0-562763105ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./data/MOT17_tiny.zip\n",
      "   creating: ./data/MOT17_tiny/\n",
      "   creating: ./data/MOT17_tiny/test/\n",
      "   creating: ./data/MOT17_tiny/train/\n",
      "   creating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/\n",
      "   creating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/det/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/det/det.txt  \n",
      "   creating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/gt/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/gt/gt.txt  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/gt/gt_half-train.txt  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/gt/gt_half-val.txt  \n",
      "   creating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000001.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000002.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000003.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000004.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000005.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000006.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000007.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000008.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000009.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000010.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000011.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000012.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000013.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000014.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000015.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000016.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000017.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000018.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000019.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000020.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000021.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000022.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000023.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000024.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000025.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000026.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000027.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000028.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000029.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000030.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000031.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000032.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000033.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000034.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000035.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000036.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000037.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000038.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000039.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000040.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000041.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000042.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000043.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000044.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000045.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000046.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000047.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000048.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000049.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000050.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000051.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000052.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000053.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000054.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000055.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000056.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000057.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000058.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000059.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000060.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000061.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000062.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000063.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000064.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000065.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000066.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000067.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000068.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000069.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000070.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000071.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000072.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000073.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000074.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000075.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000076.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000077.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000078.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000079.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000080.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000081.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000082.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000083.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000084.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000085.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000086.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000087.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000088.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000089.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000090.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000091.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000092.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000093.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000094.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000095.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000096.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000097.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000098.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000099.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000100.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000101.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000102.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000103.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000104.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000105.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000106.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000107.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000108.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000109.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000110.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000111.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000112.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000113.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000114.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000115.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000116.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000117.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000118.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000119.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000120.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000121.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000122.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000123.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000124.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000125.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000126.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000127.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000128.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000129.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000130.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000131.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000132.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000133.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000134.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000135.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000136.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000137.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000138.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000139.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000140.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000141.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000142.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000143.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000144.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000145.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000146.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000147.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000148.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000149.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000150.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000151.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000152.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000153.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000154.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000155.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000156.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000157.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000158.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000159.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000160.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000161.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000162.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000163.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000164.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000165.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000166.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000167.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000168.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000169.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000170.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000171.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000172.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000173.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000174.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000175.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000176.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000177.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000178.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000179.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000180.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000181.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000182.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000183.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000184.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000185.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000186.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000187.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000188.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000189.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000190.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000191.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000192.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000193.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000194.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000195.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000196.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000197.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000198.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000199.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000200.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000201.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000202.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000203.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000204.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000205.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000206.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000207.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000208.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000209.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000210.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000211.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000212.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000213.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000214.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000215.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000216.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000217.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000218.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000219.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000220.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000221.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000222.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000223.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000224.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000225.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000226.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000227.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000228.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000229.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000230.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000231.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000232.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000233.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000234.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000235.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000236.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000237.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000238.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000239.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000240.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000241.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000242.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000243.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000244.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000245.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000246.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000247.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000248.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000249.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000250.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000251.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000252.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000253.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000254.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000255.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000256.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000257.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000258.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000259.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000260.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000261.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000262.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000263.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000264.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000265.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000266.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000267.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000268.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000269.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000270.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000271.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000272.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000273.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000274.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000275.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000276.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000277.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000278.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000279.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000280.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000281.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000282.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000283.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000284.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000285.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000286.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000287.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000288.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000289.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000290.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000291.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000292.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000293.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000294.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000295.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000296.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000297.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000298.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000299.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000300.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000301.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000302.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000303.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000304.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000305.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000306.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000307.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000308.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000309.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000310.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000311.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000312.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000313.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000314.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000315.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000316.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000317.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000318.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000319.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000320.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000321.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000322.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000323.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000324.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000325.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000326.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000327.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000328.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000329.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000330.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000331.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000332.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000333.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000334.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000335.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000336.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000337.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000338.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000339.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000340.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000341.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000342.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000343.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000344.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000345.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000346.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000347.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000348.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000349.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000350.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000351.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000352.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000353.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000354.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000355.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000356.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000357.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000358.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000359.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000360.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000361.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000362.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000363.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000364.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000365.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000366.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000367.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000368.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000369.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000370.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000371.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000372.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000373.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000374.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000375.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000376.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000377.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000378.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000379.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000380.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000381.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000382.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000383.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000384.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000385.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000386.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000387.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000388.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000389.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000390.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000391.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000392.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000393.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000394.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000395.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000396.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000397.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000398.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000399.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000400.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000401.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000402.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000403.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000404.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000405.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000406.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000407.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000408.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000409.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000410.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000411.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000412.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000413.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000414.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000415.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000416.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000417.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000418.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000419.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000420.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000421.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000422.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000423.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000424.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000425.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000426.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000427.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000428.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000429.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000430.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000431.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000432.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000433.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000434.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000435.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000436.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000437.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000438.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000439.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000440.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000441.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000442.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000443.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000444.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000445.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000446.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000447.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000448.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000449.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000450.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000451.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000452.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000453.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000454.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000455.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000456.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000457.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000458.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000459.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000460.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000461.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000462.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000463.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000464.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000465.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000466.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000467.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000468.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000469.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000470.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000471.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000472.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000473.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000474.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000475.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000476.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000477.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000478.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000479.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000480.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000481.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000482.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000483.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000484.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000485.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000486.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000487.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000488.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000489.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000490.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000491.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000492.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000493.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000494.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000495.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000496.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000497.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000498.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000499.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000500.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000501.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000502.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000503.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000504.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000505.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000506.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000507.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000508.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000509.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000510.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000511.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000512.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000513.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000514.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000515.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000516.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000517.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000518.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000519.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000520.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000521.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000522.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000523.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000524.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000525.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000526.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000527.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000528.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000529.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000530.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000531.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000532.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000533.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000534.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000535.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000536.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000537.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000538.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000539.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000540.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000541.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000542.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000543.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000544.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000545.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000546.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000547.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000548.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000549.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000550.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000551.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000552.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000553.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000554.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000555.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000556.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000557.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000558.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000559.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000560.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000561.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000562.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000563.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000564.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000565.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000566.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000567.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000568.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000569.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000570.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000571.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000572.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000573.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000574.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000575.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000576.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000577.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000578.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000579.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000580.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000581.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000582.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000583.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000584.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000585.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000586.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000587.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000588.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000589.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000590.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000591.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000592.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000593.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000594.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000595.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000596.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000597.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000598.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000599.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000600.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/seqinfo.ini  \n",
      "   creating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/\n",
      "   creating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/det/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/det/det.txt  \n",
      "   creating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/gt/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/gt/gt.txt  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/gt/gt_half-train.txt  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/gt/gt_half-val.txt  \n",
      "   creating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000001.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000002.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000003.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000004.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000005.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000006.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000007.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000008.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000009.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000010.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000011.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000012.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000013.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000014.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000015.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000016.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000017.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000018.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000019.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000020.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000021.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000022.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000023.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000024.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000025.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000026.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000027.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000028.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000029.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000030.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000031.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000032.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000033.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000034.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000035.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000036.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000037.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000038.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000039.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000040.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000041.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000042.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000043.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000044.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000045.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000046.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000047.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000048.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000049.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000050.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000051.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000052.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000053.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000054.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000055.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000056.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000057.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000058.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000059.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000060.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000061.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000062.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000063.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000064.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000065.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000066.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000067.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000068.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000069.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000070.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000071.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000072.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000073.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000074.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000075.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000076.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000077.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000078.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000079.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000080.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000081.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000082.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000083.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000084.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000085.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000086.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000087.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000088.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000089.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000090.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000091.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000092.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000093.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000094.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000095.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000096.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000097.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000098.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000099.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000100.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000101.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000102.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000103.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000104.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000105.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000106.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000107.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000108.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000109.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000110.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000111.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000112.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000113.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000114.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000115.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000116.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000117.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000118.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000119.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000120.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000121.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000122.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000123.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000124.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000125.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000126.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000127.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000128.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000129.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000130.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000131.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000132.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000133.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000134.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000135.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000136.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000137.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000138.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000139.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000140.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000141.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000142.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000143.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000144.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000145.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000146.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000147.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000148.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000149.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000150.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000151.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000152.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000153.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000154.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000155.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000156.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000157.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000158.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000159.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000160.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000161.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000162.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000163.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000164.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000165.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000166.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000167.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000168.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000169.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000170.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000171.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000172.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000173.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000174.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000175.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000176.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000177.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000178.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000179.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000180.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000181.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000182.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000183.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000184.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000185.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000186.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000187.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000188.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000189.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000190.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000191.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000192.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000193.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000194.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000195.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000196.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000197.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000198.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000199.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000200.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000201.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000202.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000203.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000204.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000205.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000206.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000207.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000208.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000209.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000210.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000211.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000212.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000213.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000214.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000215.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000216.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000217.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000218.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000219.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000220.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000221.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000222.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000223.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000224.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000225.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000226.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000227.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000228.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000229.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000230.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000231.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000232.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000233.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000234.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000235.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000236.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000237.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000238.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000239.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000240.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000241.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000242.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000243.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000244.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000245.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000246.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000247.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000248.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000249.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000250.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000251.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000252.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000253.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000254.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000255.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000256.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000257.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000258.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000259.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000260.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000261.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000262.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000263.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000264.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000265.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000266.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000267.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000268.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000269.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000270.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000271.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000272.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000273.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000274.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000275.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000276.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000277.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000278.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000279.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000280.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000281.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000282.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000283.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000284.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000285.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000286.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000287.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000288.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000289.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000290.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000291.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000292.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000293.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000294.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000295.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000296.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000297.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000298.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000299.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000300.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000301.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000302.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000303.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000304.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000305.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000306.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000307.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000308.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000309.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000310.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000311.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000312.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000313.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000314.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000315.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000316.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000317.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000318.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000319.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000320.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000321.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000322.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000323.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000324.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000325.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000326.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000327.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000328.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000329.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000330.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000331.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000332.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000333.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000334.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000335.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000336.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000337.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000338.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000339.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000340.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000341.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000342.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000343.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000344.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000345.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000346.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000347.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000348.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000349.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000350.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000351.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000352.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000353.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000354.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000355.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000356.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000357.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000358.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000359.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000360.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000361.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000362.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000363.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000364.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000365.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000366.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000367.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000368.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000369.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000370.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000371.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000372.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000373.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000374.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000375.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000376.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000377.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000378.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000379.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000380.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000381.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000382.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000383.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000384.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000385.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000386.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000387.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000388.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000389.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000390.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000391.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000392.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000393.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000394.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000395.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000396.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000397.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000398.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000399.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000400.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000401.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000402.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000403.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000404.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000405.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000406.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000407.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000408.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000409.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000410.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000411.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000412.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000413.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000414.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000415.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000416.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000417.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000418.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000419.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000420.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000421.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000422.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000423.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000424.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000425.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000426.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000427.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000428.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000429.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000430.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000431.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000432.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000433.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000434.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000435.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000436.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000437.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000438.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000439.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000440.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000441.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000442.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000443.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000444.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000445.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000446.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000447.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000448.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000449.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000450.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000451.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000452.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000453.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000454.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000455.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000456.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000457.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000458.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000459.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000460.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000461.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000462.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000463.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000464.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000465.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000466.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000467.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000468.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000469.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000470.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000471.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000472.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000473.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000474.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000475.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000476.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000477.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000478.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000479.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000480.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000481.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000482.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000483.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000484.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000485.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000486.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000487.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000488.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000489.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000490.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000491.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000492.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000493.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000494.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000495.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000496.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000497.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000498.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000499.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000500.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000501.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000502.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000503.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000504.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000505.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000506.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000507.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000508.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000509.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000510.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000511.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000512.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000513.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000514.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000515.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000516.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000517.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000518.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000519.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000520.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000521.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000522.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000523.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000524.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000525.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000526.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000527.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000528.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000529.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000530.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000531.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000532.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000533.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000534.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000535.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000536.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000537.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000538.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000539.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000540.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000541.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000542.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000543.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000544.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000545.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000546.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000547.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000548.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000549.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000550.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000551.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000552.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000553.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000554.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000555.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000556.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000557.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000558.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000559.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000560.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000561.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000562.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000563.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000564.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000565.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000566.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000567.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000568.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000569.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000570.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000571.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000572.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000573.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000574.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000575.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000576.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000577.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000578.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000579.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000580.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000581.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000582.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000583.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000584.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000585.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000586.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000587.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000588.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000589.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000590.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000591.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000592.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000593.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000594.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000595.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000596.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000597.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000598.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000599.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000600.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000601.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000602.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000603.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000604.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000605.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000606.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000607.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000608.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000609.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000610.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000611.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000612.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000613.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000614.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000615.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000616.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000617.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000618.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000619.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000620.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000621.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000622.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000623.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000624.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000625.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000626.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000627.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000628.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000629.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000630.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000631.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000632.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000633.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000634.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000635.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000636.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000637.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000638.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000639.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000640.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000641.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000642.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000643.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000644.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000645.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000646.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000647.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000648.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000649.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000650.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000651.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000652.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000653.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000654.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000655.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000656.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000657.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000658.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000659.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000660.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000661.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000662.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000663.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000664.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000665.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000666.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000667.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000668.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000669.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000670.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000671.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000672.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000673.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000674.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000675.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000676.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000677.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000678.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000679.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000680.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000681.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000682.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000683.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000684.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000685.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000686.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000687.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000688.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000689.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000690.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000691.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000692.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000693.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000694.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000695.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000696.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000697.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000698.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000699.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000700.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000701.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000702.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000703.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000704.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000705.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000706.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000707.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000708.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000709.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000710.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000711.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000712.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000713.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000714.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000715.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000716.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000717.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000718.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000719.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000720.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000721.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000722.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000723.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000724.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000725.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000726.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000727.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000728.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000729.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000730.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000731.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000732.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000733.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000734.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000735.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000736.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000737.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000738.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000739.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000740.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000741.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000742.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000743.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000744.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000745.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000746.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000747.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000748.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000749.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000750.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000751.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000752.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000753.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000754.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000755.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000756.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000757.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000758.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000759.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000760.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000761.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000762.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000763.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000764.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000765.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000766.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000767.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000768.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000769.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000770.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000771.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000772.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000773.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000774.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000775.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000776.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000777.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000778.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000779.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000780.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000781.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000782.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000783.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000784.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000785.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000786.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000787.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000788.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000789.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000790.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000791.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000792.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000793.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000794.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000795.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000796.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000797.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000798.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000799.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000800.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000801.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000802.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000803.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000804.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000805.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000806.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000807.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000808.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000809.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000810.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000811.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000812.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000813.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000814.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000815.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000816.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000817.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000818.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000819.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000820.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000821.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000822.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000823.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000824.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000825.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000826.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000827.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000828.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000829.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000830.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000831.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000832.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000833.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000834.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000835.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000836.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000837.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000838.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000839.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000840.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000841.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000842.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000843.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000844.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000845.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000846.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000847.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000848.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000849.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000850.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000851.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000852.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000853.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000854.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000855.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000856.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000857.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000858.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000859.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000860.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000861.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000862.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000863.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000864.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000865.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000866.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000867.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000868.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000869.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000870.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000871.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000872.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000873.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000874.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000875.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000876.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000877.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000878.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000879.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000880.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000881.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000882.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000883.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000884.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000885.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000886.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000887.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000888.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000889.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000890.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000891.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000892.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000893.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000894.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000895.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000896.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000897.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000898.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000899.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000900.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000901.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000902.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000903.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000904.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000905.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000906.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000907.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000908.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000909.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000910.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000911.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000912.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000913.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000914.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000915.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000916.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000917.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000918.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000919.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000920.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000921.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000922.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000923.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000924.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000925.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000926.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000927.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000928.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000929.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000930.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000931.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000932.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000933.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000934.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000935.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000936.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000937.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000938.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000939.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000940.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000941.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000942.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000943.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000944.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000945.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000946.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000947.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000948.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000949.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000950.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000951.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000952.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000953.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000954.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000955.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000956.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000957.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000958.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000959.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000960.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000961.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000962.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000963.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000964.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000965.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000966.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000967.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000968.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000969.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000970.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000971.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000972.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000973.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000974.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000975.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000976.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000977.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000978.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000979.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000980.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000981.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000982.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000983.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000984.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000985.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000986.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000987.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000988.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000989.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000990.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000991.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000992.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000993.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000994.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000995.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000996.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000997.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000998.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000999.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001000.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001001.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001002.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001003.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001004.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001005.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001006.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001007.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001008.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001009.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001010.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001011.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001012.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001013.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001014.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001015.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001016.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001017.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001018.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001019.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001020.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001021.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001022.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001023.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001024.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001025.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001026.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001027.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001028.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001029.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001030.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001031.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001032.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001033.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001034.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001035.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001036.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001037.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001038.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001039.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001040.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001041.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001042.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001043.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001044.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001045.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001046.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001047.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001048.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001049.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001050.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/seqinfo.ini  \n"
     ]
    }
   ],
   "source": [
    "# 解压\n",
    "!rm -rf data/MOT17_tiny\n",
    "!unzip ./data/MOT17_tiny.zip -d ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd41fd-195a-4e19-8404-d508ddaf286f",
   "metadata": {},
   "source": [
    "## 将 txt 格式的标注文件，转换为 coco 格式的 json 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372314b8-a9b7-406e-9423-9faf4aea005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train set to COCO format\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.84it/s]\n",
      "train has 145 instances.\n",
      "Done! Saved as ./data/MOT17_tiny/annotations/train_cocoformat.json and ./data/MOT17_tiny/annotations/train_detections.pkl\n",
      "Converting test set to COCO format\n",
      "0it [00:00, ?it/s]\n",
      "test has 0 instances.\n",
      "Done! Saved as ./data/MOT17_tiny/annotations/test_cocoformat.json and ./data/MOT17_tiny/annotations/test_detections.pkl\n",
      "Converting half-train set to COCO format\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.04it/s]\n",
      "half-train has 104 instances.\n",
      "Done! Saved as ./data/MOT17_tiny/annotations/half-train_cocoformat.json and ./data/MOT17_tiny/annotations/half-train_detections.pkl\n",
      "Converting half-val set to COCO format\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.00it/s]\n",
      "half-val has 122 instances.\n",
      "Done! Saved as ./data/MOT17_tiny/annotations/half-val_cocoformat.json and ./data/MOT17_tiny/annotations/half-val_detections.pkl\n"
     ]
    }
   ],
   "source": [
    "!python ./tools/convert_datasets/mot/mot2coco.py -i ./data/MOT17_tiny/ -o ./data/MOT17_tiny/annotations --split-train --convert-det"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb571e71-0ae6-4e14-829c-a0e2c1102c9a",
   "metadata": {},
   "source": [
    "## 从数据集中把行人的图块裁剪出来，用于训练 ReID 重识别模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43f952e-d06f-46da-a590-53a6d6b33d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除已有的 reid 目录（如有）\n",
    "!rm -rf ./data/MOT17_tiny/reid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f6fae6b-a8cf-43f6-81fb-a21306a99dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 2/2 [17:50<00:00, 535.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# 大概需要 20 分钟\n",
    "!python ./tools/convert_datasets/mot/mot2reid.py -i ./data/MOT17_tiny/ -o ./data/MOT17_tiny/reid --val-split 0.9 --vis-threshold 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae56b7-f89d-44f2-8898-4c4876114346",
   "metadata": {},
   "source": [
    "## 训练 ByteTrack 多目标追踪算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe76e6b0-e600-4a11-b06c-7d5f0326d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('mmtracking')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6ff5d-ed11-4360-bf07-6b7e7b4e404a",
   "metadata": {},
   "source": [
    "### 下载 config 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce2b820-e467-4a7c-854c-742577d83235",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://zihao-openmmlab.obs.cn-east-3.myhuaweicloud.com/20220418-mmtracking/bytetrack_yolox_x_crowdhuman_mot17-private-half-zihao.py -O configs/mot/bytetrack/bytetrack_yolox_x_crowdhuman_mot17-private-half-zihao.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62e349-6e9a-4490-8966-46ce6f08c2e9",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0494a4-4d4f-4058-a25d-8ac1b8a78da3",
   "metadata": {},
   "source": [
    "将MOT17_tiny 改为 MOT17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ec6c9d9-9ca9-4237-bb36-34c9d9cc9cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/featurize/work/MMTracking教程/0420/mmtracking/mmtrack/core/utils/misc.py:25: UserWarning: Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n",
      "  f'Setting OMP_NUM_THREADS environment variable for each process '\n",
      "/home/featurize/work/MMTracking教程/0420/mmtracking/mmtrack/core/utils/misc.py:35: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n",
      "  f'Setting MKL_NUM_THREADS environment variable for each process '\n",
      "2022-04-21 20:30:21,568 - mmtrack - INFO - Environment info:\n",
      "------------------------------------------------------------\n",
      "sys.platform: linux\n",
      "Python: 3.7.10 (default, Jun  4 2021, 14:48:32) [GCC 7.5.0]\n",
      "CUDA available: True\n",
      "GPU 0: NVIDIA GeForce GTX 1080 Ti\n",
      "CUDA_HOME: /usr/local/cuda\n",
      "NVCC: Build cuda_11.2.r11.2/compiler.29618528_0\n",
      "GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\n",
      "PyTorch: 1.10.1+cu113\n",
      "PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.3\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.2\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "TorchVision: 0.11.2+cu113\n",
      "OpenCV: 4.5.4\n",
      "MMCV: 1.4.8\n",
      "MMCV Compiler: GCC 7.3\n",
      "MMCV CUDA Compiler: 11.3\n",
      "MMTracking: 0.12.0+12634be\n",
      "------------------------------------------------------------\n",
      "\n",
      "2022-04-21 20:30:21,569 - mmtrack - INFO - Distributed training: False\n",
      "2022-04-21 20:30:22,540 - mmtrack - INFO - Config:\n",
      "img_scale = (800, 1440)\n",
      "model = dict(\n",
      "    detector=dict(\n",
      "        type='YOLOX',\n",
      "        input_size=(800, 1440),\n",
      "        random_size_range=(18, 32),\n",
      "        random_size_interval=10,\n",
      "        backbone=dict(\n",
      "            type='CSPDarknet', deepen_factor=1.33, widen_factor=1.25),\n",
      "        neck=dict(\n",
      "            type='YOLOXPAFPN',\n",
      "            in_channels=[320, 640, 1280],\n",
      "            out_channels=320,\n",
      "            num_csp_blocks=4),\n",
      "        bbox_head=dict(\n",
      "            type='YOLOXHead',\n",
      "            num_classes=1,\n",
      "            in_channels=320,\n",
      "            feat_channels=320),\n",
      "        train_cfg=dict(\n",
      "            assigner=dict(type='SimOTAAssigner', center_radius=2.5)),\n",
      "        test_cfg=dict(score_thr=0.01, nms=dict(type='nms', iou_threshold=0.7)),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint=\n",
      "            'https://download.openmmlab.com/mmdetection/v2.0/yolox/yolox_x_8x8_300e_coco/yolox_x_8x8_300e_coco_20211126_140254-1ef88d67.pth'\n",
      "        )),\n",
      "    type='ByteTrack',\n",
      "    motion=dict(type='KalmanFilter'),\n",
      "    tracker=dict(\n",
      "        type='ByteTracker',\n",
      "        obj_score_thrs=dict(high=0.6, low=0.1),\n",
      "        init_track_thr=0.7,\n",
      "        weight_iou_with_det_scores=True,\n",
      "        match_iou_thrs=dict(high=0.1, low=0.5, tentative=0.3),\n",
      "        num_frames_retain=30))\n",
      "dataset_type = 'MOTChallengeDataset'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(\n",
      "        type='Mosaic',\n",
      "        img_scale=(800, 1440),\n",
      "        pad_val=114.0,\n",
      "        bbox_clip_border=False),\n",
      "    dict(\n",
      "        type='RandomAffine',\n",
      "        scaling_ratio_range=(0.1, 2),\n",
      "        border=(-400, -720),\n",
      "        bbox_clip_border=False),\n",
      "    dict(\n",
      "        type='MixUp',\n",
      "        img_scale=(800, 1440),\n",
      "        ratio_range=(0.8, 1.6),\n",
      "        pad_val=114.0,\n",
      "        bbox_clip_border=False),\n",
      "    dict(type='YOLOXHSVRandomAug'),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Resize',\n",
      "        img_scale=(800, 1440),\n",
      "        keep_ratio=True,\n",
      "        bbox_clip_border=False),\n",
      "    dict(type='Pad', size_divisor=32, pad_val=dict(img=(114.0, 114.0, 114.0))),\n",
      "    dict(type='FilterAnnotations', min_gt_bbox_wh=(1, 1), keep_empty=False),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(800, 1440),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[0.0, 0.0, 0.0],\n",
      "                std=[1.0, 1.0, 1.0],\n",
      "                to_rgb=False),\n",
      "            dict(\n",
      "                type='Pad',\n",
      "                size_divisor=32,\n",
      "                pad_val=dict(img=(114.0, 114.0, 114.0))),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='VideoCollect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data_root = 'data/MOT17/'\n",
      "data = dict(\n",
      "    samples_per_gpu=4,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='MultiImageMixDataset',\n",
      "        dataset=dict(\n",
      "            type='CocoDataset',\n",
      "            ann_file=['data/MOT17/annotations/half-train_cocoformat.json'],\n",
      "            img_prefix=['data/MOT17/train'],\n",
      "            classes=('pedestrian', ),\n",
      "            pipeline=[\n",
      "                dict(type='LoadImageFromFile'),\n",
      "                dict(type='LoadAnnotations', with_bbox=True)\n",
      "            ],\n",
      "            filter_empty_gt=False),\n",
      "        pipeline=[\n",
      "            dict(\n",
      "                type='Mosaic',\n",
      "                img_scale=(800, 1440),\n",
      "                pad_val=114.0,\n",
      "                bbox_clip_border=False),\n",
      "            dict(\n",
      "                type='RandomAffine',\n",
      "                scaling_ratio_range=(0.1, 2),\n",
      "                border=(-400, -720),\n",
      "                bbox_clip_border=False),\n",
      "            dict(\n",
      "                type='MixUp',\n",
      "                img_scale=(800, 1440),\n",
      "                ratio_range=(0.8, 1.6),\n",
      "                pad_val=114.0,\n",
      "                bbox_clip_border=False),\n",
      "            dict(type='YOLOXHSVRandomAug'),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Resize',\n",
      "                img_scale=(800, 1440),\n",
      "                keep_ratio=True,\n",
      "                bbox_clip_border=False),\n",
      "            dict(\n",
      "                type='Pad',\n",
      "                size_divisor=32,\n",
      "                pad_val=dict(img=(114.0, 114.0, 114.0))),\n",
      "            dict(\n",
      "                type='FilterAnnotations',\n",
      "                min_gt_bbox_wh=(1, 1),\n",
      "                keep_empty=False),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='MOTChallengeDataset',\n",
      "        ann_file='data/MOT17/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17/train',\n",
      "        ref_img_sampler=None,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(800, 1440),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[0.0, 0.0, 0.0],\n",
      "                        std=[1.0, 1.0, 1.0],\n",
      "                        to_rgb=False),\n",
      "                    dict(\n",
      "                        type='Pad',\n",
      "                        size_divisor=32,\n",
      "                        pad_val=dict(img=(114.0, 114.0, 114.0))),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='VideoCollect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        interpolate_tracks_cfg=dict(min_num_frames=5, max_num_frames=20)),\n",
      "    test=dict(\n",
      "        type='MOTChallengeDataset',\n",
      "        ann_file='data/MOT17/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17/train',\n",
      "        ref_img_sampler=None,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(800, 1440),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[0.0, 0.0, 0.0],\n",
      "                        std=[1.0, 1.0, 1.0],\n",
      "                        to_rgb=False),\n",
      "                    dict(\n",
      "                        type='Pad',\n",
      "                        size_divisor=32,\n",
      "                        pad_val=dict(img=(114.0, 114.0, 114.0))),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='VideoCollect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        interpolate_tracks_cfg=dict(min_num_frames=5, max_num_frames=20)),\n",
      "    persistent_workers=True)\n",
      "optimizer = dict(\n",
      "    type='SGD',\n",
      "    lr=0.0005,\n",
      "    momentum=0.9,\n",
      "    weight_decay=0.0005,\n",
      "    nesterov=True,\n",
      "    paramwise_cfg=dict(norm_decay_mult=0.0, bias_decay_mult=0.0))\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "samples_per_gpu = 4\n",
      "total_epochs = 2\n",
      "num_last_epochs = 10\n",
      "interval = 5\n",
      "lr_config = dict(\n",
      "    policy='YOLOX',\n",
      "    warmup='exp',\n",
      "    by_epoch=False,\n",
      "    warmup_by_epoch=True,\n",
      "    warmup_ratio=1,\n",
      "    warmup_iters=1,\n",
      "    num_last_epochs=10,\n",
      "    min_lr_ratio=0.05)\n",
      "custom_hooks = [\n",
      "    dict(type='YOLOXModeSwitchHook', num_last_epochs=10, priority=48),\n",
      "    dict(type='SyncNormHook', num_last_epochs=10, interval=5, priority=48),\n",
      "    dict(\n",
      "        type='ExpMomentumEMAHook',\n",
      "        resume_from=None,\n",
      "        momentum=0.0001,\n",
      "        priority=49)\n",
      "]\n",
      "evaluation = dict(metric=['bbox', 'track'], interval=1)\n",
      "search_metrics = ['MOTA', 'IDF1', 'FN', 'FP', 'IDs', 'MT', 'ML']\n",
      "fp16 = dict(loss_scale=dict(init_scale=512.0))\n",
      "work_dir = 'bytetrack_zihao'\n",
      "gpu_ids = [0]\n",
      "\n",
      "2022-04-21 20:30:22,541 - mmtrack - INFO - Set random seed to 1575990438, deterministic: False\n",
      "2022-04-21 20:30:23,999 - mmdet - INFO - image shape: height=800, width=1440 in YOLOX.__init__\n",
      "2022-04-21 20:30:24,036 - mmtrack - INFO - initialize YOLOX with init_cfg {'type': 'Pretrained', 'checkpoint': 'https://download.openmmlab.com/mmdetection/v2.0/yolox/yolox_x_8x8_300e_coco/yolox_x_8x8_300e_coco_20211126_140254-1ef88d67.pth'}\n",
      "2022-04-21 20:30:24,036 - mmcv - INFO - load model from: https://download.openmmlab.com/mmdetection/v2.0/yolox/yolox_x_8x8_300e_coco/yolox_x_8x8_300e_coco_20211126_140254-1ef88d67.pth\n",
      "2022-04-21 20:30:24,037 - mmcv - INFO - load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/yolox/yolox_x_8x8_300e_coco/yolox_x_8x8_300e_coco_20211126_140254-1ef88d67.pth\n",
      "2022-04-21 20:30:24,567 - mmcv - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for bbox_head.multi_level_conv_cls.0.weight: copying a param with shape torch.Size([80, 320, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 320, 1, 1]).\n",
      "size mismatch for bbox_head.multi_level_conv_cls.0.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([1]).\n",
      "size mismatch for bbox_head.multi_level_conv_cls.1.weight: copying a param with shape torch.Size([80, 320, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 320, 1, 1]).\n",
      "size mismatch for bbox_head.multi_level_conv_cls.1.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([1]).\n",
      "size mismatch for bbox_head.multi_level_conv_cls.2.weight: copying a param with shape torch.Size([80, 320, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 320, 1, 1]).\n",
      "size mismatch for bbox_head.multi_level_conv_cls.2.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([1]).\n",
      "loading annotations into memory...\n",
      "Done (t=0.45s)\n",
      "creating index...\n",
      "index created!\n",
      "2022-04-21 20:30:25,121 - mmdet - INFO - image shape: height=800, width=1440 in Mosaic.__init__\n",
      "2022-04-21 20:30:25,122 - mmdet - INFO - image shape: height=800, width=1440 in MixUp.__init__\n",
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "2022-04-21 20:30:30,142 - mmtrack - INFO - Start running, host: featurize@featurize, work_dir: /home/featurize/work/MMTracking教程/0420/mmtracking/bytetrack_zihao\n",
      "2022-04-21 20:30:30,143 - mmtrack - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) YOLOXLrUpdaterHook                 \n",
      "(ABOVE_NORMAL) Fp16OptimizerHook                  \n",
      "(49          ) ExpMomentumEMAHook                 \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) YOLOXLrUpdaterHook                 \n",
      "(48          ) YOLOXModeSwitchHook                \n",
      "(48          ) SyncNormHook                       \n",
      "(49          ) ExpMomentumEMAHook                 \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) YOLOXLrUpdaterHook                 \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) Fp16OptimizerHook                  \n",
      "(49          ) ExpMomentumEMAHook                 \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(48          ) SyncNormHook                       \n",
      "(49          ) ExpMomentumEMAHook                 \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-04-21 20:30:30,143 - mmtrack - INFO - workflow: [('train', 1)], max: 2 epochs\n",
      "2022-04-21 20:30:30,187 - mmtrack - INFO - Checkpoints will be saved to /home/featurize/work/MMTracking教程/0420/mmtracking/bytetrack_zihao by HardDiskBackend.\n",
      "Traceback (most recent call last):\n",
      "  File \"tools/train.py\", line 210, in <module>\n",
      "    main()\n",
      "  File \"tools/train.py\", line 206, in main\n",
      "    meta=meta)\n",
      "  File \"/home/featurize/work/MMTracking教程/0420/mmtracking/mmtrack/apis/train.py\", line 175, in train_model\n",
      "    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py\", line 127, in run\n",
      "    epoch_runner(data_loaders[i], **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py\", line 50, in train\n",
      "    self.run_iter(data_batch, train_mode=True, **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py\", line 30, in run_iter\n",
      "    **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmcv/parallel/data_parallel.py\", line 75, in train_step\n",
      "    return self.module.train_step(*inputs[0], **kwargs[0])\n",
      "  File \"/home/featurize/work/MMTracking教程/0420/mmtracking/mmtrack/models/mot/base.py\", line 200, in train_step\n",
      "    losses = self(**data)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmcv/runner/fp16_utils.py\", line 139, in new_func\n",
      "    output = old_func(*new_args, **new_kwargs)\n",
      "  File \"/home/featurize/work/MMTracking教程/0420/mmtracking/mmtrack/models/mot/base.py\", line 134, in forward\n",
      "    return self.forward_train(img, img_metas, **kwargs)\n",
      "  File \"/home/featurize/work/MMTracking教程/0420/mmtracking/mmtrack/models/mot/byte_track.py\", line 43, in forward_train\n",
      "    return self.detector.forward_train(*args, **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmdet/models/detectors/yolox.py\", line 96, in forward_train\n",
      "    gt_labels, gt_bboxes_ignore)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmdet/models/detectors/single_stage.py\", line 82, in forward_train\n",
      "    x = self.extract_feat(img)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmdet/models/detectors/single_stage.py\", line 45, in extract_feat\n",
      "    x = self.neck(x)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmdet/models/necks/yolox_pafpn.py\", line 139, in forward\n",
      "    torch.cat([upsample_feat, feat_low], 1))\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmdet/models/utils/csp_layer.py\", line 147, in forward\n",
      "    x_main = self.blocks(x_main)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmdet/models/utils/csp_layer.py\", line 67, in forward\n",
      "    out = self.conv2(out)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/mmcv/cnn/bricks/conv_module.py\", line 203, in forward\n",
      "    x = self.norm(x)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\", line 179, in forward\n",
      "    self.eps,\n",
      "  File \"/environment/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\", line 2283, in batch_norm\n",
      "    input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 10.92 GiB total capacity; 9.75 GiB already allocated; 11.38 MiB free; 9.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python tools/train.py \\\n",
    "        configs/mot/bytetrack/bytetrack_yolox_x_crowdhuman_mot17-private-half-zihao.py \\\n",
    "        --work-dir bytetrack_zihao \\\n",
    "        --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555704f-df63-407f-867e-d44e7eff3ee8",
   "metadata": {},
   "source": [
    "### 用训练得到的模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29264628-9336-4503-aaf3-7157631896dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354629d-f926-4bb5-badd-db8d655139a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e474e6-ad89-445f-88b9-4bbedb3dc652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1364b76-ac1e-4da2-bce6-0f06527fc3f0",
   "metadata": {},
   "source": [
    "## 训练多目标追踪模型的目标检测算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6b92d3-80bc-4253-bf59-27d4811586a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "cfg = mmcv.Config.fromfile('./configs/det/faster-rcnn_r50_fpn_4e_mot17-half.py')\n",
    "cfg.data_root = 'data/MOT17_tiny/'\n",
    "cfg.data.test.ann_file = cfg.data.test.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.ann_file = cfg.data.train.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.val.ann_file = cfg.data.val.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "cfg.data.test.img_prefix = cfg.data.test.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.img_prefix = cfg.data.train.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.val.img_prefix = cfg.data.val.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "cfg.work_dir = './tutorial_exps/detector'\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e78c3273-be10-45c7-80a0-c769899edfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = dict(\n",
      "    detector=dict(\n",
      "        type='FasterRCNN',\n",
      "        backbone=dict(\n",
      "            type='ResNet',\n",
      "            depth=50,\n",
      "            num_stages=4,\n",
      "            out_indices=(0, 1, 2, 3),\n",
      "            frozen_stages=1,\n",
      "            norm_cfg=dict(type='BN', requires_grad=True),\n",
      "            norm_eval=True,\n",
      "            style='pytorch',\n",
      "            init_cfg=dict(\n",
      "                type='Pretrained', checkpoint='torchvision://resnet50')),\n",
      "        neck=dict(\n",
      "            type='FPN',\n",
      "            in_channels=[256, 512, 1024, 2048],\n",
      "            out_channels=256,\n",
      "            num_outs=5),\n",
      "        rpn_head=dict(\n",
      "            type='RPNHead',\n",
      "            in_channels=256,\n",
      "            feat_channels=256,\n",
      "            anchor_generator=dict(\n",
      "                type='AnchorGenerator',\n",
      "                scales=[8],\n",
      "                ratios=[0.5, 1.0, 2.0],\n",
      "                strides=[4, 8, 16, 32, 64]),\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[1.0, 1.0, 1.0, 1.0],\n",
      "                clip_border=False),\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "            loss_bbox=dict(\n",
      "                type='SmoothL1Loss', beta=0.1111111111111111,\n",
      "                loss_weight=1.0)),\n",
      "        roi_head=dict(\n",
      "            type='StandardRoIHead',\n",
      "            bbox_roi_extractor=dict(\n",
      "                type='SingleRoIExtractor',\n",
      "                roi_layer=dict(\n",
      "                    type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "                out_channels=256,\n",
      "                featmap_strides=[4, 8, 16, 32]),\n",
      "            bbox_head=dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=1,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.1, 0.1, 0.2, 0.2],\n",
      "                    clip_border=False),\n",
      "                reg_class_agnostic=False,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', loss_weight=1.0))),\n",
      "        train_cfg=dict(\n",
      "            rpn=dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.7,\n",
      "                    neg_iou_thr=0.3,\n",
      "                    min_pos_iou=0.3,\n",
      "                    match_low_quality=True,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=256,\n",
      "                    pos_fraction=0.5,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=False),\n",
      "                allowed_border=-1,\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            rpn_proposal=dict(\n",
      "                nms_pre=2000,\n",
      "                max_per_img=1000,\n",
      "                nms=dict(type='nms', iou_threshold=0.7),\n",
      "                min_bbox_size=0),\n",
      "            rcnn=dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    min_pos_iou=0.5,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False)),\n",
      "        test_cfg=dict(\n",
      "            rpn=dict(\n",
      "                nms_pre=1000,\n",
      "                max_per_img=1000,\n",
      "                nms=dict(type='nms', iou_threshold=0.7),\n",
      "                min_bbox_size=0),\n",
      "            rcnn=dict(\n",
      "                score_thr=0.05,\n",
      "                nms=dict(type='nms', iou_threshold=0.5),\n",
      "                max_per_img=100)),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint=\n",
      "            'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
      "        )))\n",
      "dataset_type = 'CocoDataset'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile', to_float32=True),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        type='Resize',\n",
      "        img_scale=(1088, 1088),\n",
      "        ratio_range=(0.8, 1.2),\n",
      "        keep_ratio=True,\n",
      "        bbox_clip_border=False),\n",
      "    dict(type='PhotoMetricDistortion'),\n",
      "    dict(type='RandomCrop', crop_size=(1088, 1088), bbox_clip_border=False),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1088, 1088),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data_root = 'data/MOT17_tiny/'\n",
      "data = dict(\n",
      "    samples_per_gpu=2,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-train_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        classes=('pedestrian', ),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', to_float32=True),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                type='Resize',\n",
      "                img_scale=(1088, 1088),\n",
      "                ratio_range=(0.8, 1.2),\n",
      "                keep_ratio=True,\n",
      "                bbox_clip_border=False),\n",
      "            dict(type='PhotoMetricDistortion'),\n",
      "            dict(\n",
      "                type='RandomCrop',\n",
      "                crop_size=(1088, 1088),\n",
      "                bbox_clip_border=False),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        classes=('pedestrian', ),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1088, 1088),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        classes=('pedestrian', ),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1088, 1088),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]))\n",
      "evaluation = dict(metric=['bbox'])\n",
      "optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "USE_MMDET = True\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=100,\n",
      "    warmup_ratio=0.01,\n",
      "    step=[3])\n",
      "total_epochs = 4\n",
      "work_dir = './tutorial_exps/detector'\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1367154-8d39-4d49-8a59-9450483cbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmtrack.datasets import build_dataset\n",
    "from mmdet.apis import train_detector as train_model\n",
    "from mmdet.models import build_detector as build_model\n",
    "\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "model = build_model(cfg.model.detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2926a319-d130-4971-8478-228f9c34c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unset http_proxy\n",
    "!unset https_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74e1f9b0-9219-4884-bb61-67a3db7282b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MMDataParallel' object has no attribute 'init_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23172/3745670751.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/environment/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MMDataParallel' object has no attribute 'init_weights'"
     ]
    }
   ],
   "source": [
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5397334-f4b6-4a11-bd5c-6a3d3a871e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.29s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.7/site-packages/mmdet/apis/train.py:135: UserWarning: config is now expected to have a `runner` section, please set `runner` in your config.\n",
      "  'please set `runner` in your config.', UserWarning)\n",
      "2022-04-21 15:48:52,778 - mmdet - INFO - Start running, host: featurize@featurize, work_dir: /home/featurize/work/MMTracking教程/0420/mmtracking/tutorial_exps/detector\n",
      "2022-04-21 15:48:52,781 - mmdet - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-04-21 15:48:52,783 - mmdet - INFO - workflow: [('train', 1)], max: 4 epochs\n",
      "2022-04-21 15:48:52,784 - mmdet - INFO - Checkpoints will be saved to /home/featurize/work/MMTracking教程/0420/mmtracking/tutorial_exps/detector by HardDiskBackend.\n",
      "2022-04-21 15:49:09,609 - mmdet - INFO - Epoch [1][50/414]\tlr: 9.902e-03, eta: 0:08:58, time: 0.335, data_time: 0.051, memory: 2944, loss_rpn_cls: 0.5267, loss_rpn_bbox: 0.2303, loss_cls: 0.3924, acc: 84.2871, loss_bbox: 0.0951, loss: 1.2445\n",
      "2022-04-21 15:49:24,324 - mmdet - INFO - Epoch [1][100/414]\tlr: 1.980e-02, eta: 0:08:09, time: 0.294, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.3244, loss_rpn_bbox: 0.2197, loss_cls: 0.3745, acc: 84.3535, loss_bbox: 0.1706, loss: 1.0892\n",
      "2022-04-21 15:49:39,177 - mmdet - INFO - Epoch [1][150/414]\tlr: 2.000e-02, eta: 0:07:45, time: 0.297, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.3247, loss_rpn_bbox: 0.1956, loss_cls: 0.3960, acc: 83.1777, loss_bbox: 0.2077, loss: 1.1240\n",
      "2022-04-21 15:49:54,186 - mmdet - INFO - Epoch [1][200/414]\tlr: 2.000e-02, eta: 0:07:26, time: 0.300, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2957, loss_rpn_bbox: 0.1835, loss_cls: 0.3345, acc: 86.2227, loss_bbox: 0.1571, loss: 0.9709\n",
      "2022-04-21 15:50:09,187 - mmdet - INFO - Epoch [1][250/414]\tlr: 2.000e-02, eta: 0:07:09, time: 0.300, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2692, loss_rpn_bbox: 0.1808, loss_cls: 0.3747, acc: 83.9277, loss_bbox: 0.2277, loss: 1.0523\n",
      "2022-04-21 15:50:24,061 - mmdet - INFO - Epoch [1][300/414]\tlr: 2.000e-02, eta: 0:06:52, time: 0.297, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2431, loss_rpn_bbox: 0.1710, loss_cls: 0.3695, acc: 84.3262, loss_bbox: 0.2263, loss: 1.0099\n",
      "2022-04-21 15:50:39,107 - mmdet - INFO - Epoch [1][350/414]\tlr: 2.000e-02, eta: 0:06:36, time: 0.301, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2557, loss_rpn_bbox: 0.1628, loss_cls: 0.3676, acc: 83.9648, loss_bbox: 0.2185, loss: 1.0046\n",
      "2022-04-21 15:50:53,970 - mmdet - INFO - Epoch [1][400/414]\tlr: 2.000e-02, eta: 0:06:20, time: 0.297, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2300, loss_rpn_bbox: 0.1672, loss_cls: 0.3594, acc: 84.8008, loss_bbox: 0.2129, loss: 0.9695\n",
      "2022-04-21 15:50:57,933 - mmdet - INFO - Saving checkpoint at 1 epochs\n",
      "2022-04-21 15:51:16,088 - mmdet - INFO - Epoch [2][50/414]\tlr: 2.000e-02, eta: 0:05:54, time: 0.338, data_time: 0.050, memory: 2944, loss_rpn_cls: 0.2129, loss_rpn_bbox: 0.1620, loss_cls: 0.3434, acc: 85.2227, loss_bbox: 0.2187, loss: 0.9370\n",
      "2022-04-21 15:51:30,934 - mmdet - INFO - Epoch [2][100/414]\tlr: 2.000e-02, eta: 0:05:39, time: 0.297, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2237, loss_rpn_bbox: 0.1697, loss_cls: 0.3346, acc: 85.6445, loss_bbox: 0.2139, loss: 0.9419\n",
      "2022-04-21 15:51:45,755 - mmdet - INFO - Epoch [2][150/414]\tlr: 2.000e-02, eta: 0:05:24, time: 0.297, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.1925, loss_rpn_bbox: 0.1540, loss_cls: 0.3376, acc: 85.6562, loss_bbox: 0.2161, loss: 0.9002\n",
      "2022-04-21 15:52:00,466 - mmdet - INFO - Epoch [2][200/414]\tlr: 2.000e-02, eta: 0:05:09, time: 0.294, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2249, loss_rpn_bbox: 0.1468, loss_cls: 0.3104, acc: 86.8789, loss_bbox: 0.2058, loss: 0.8879\n",
      "2022-04-21 15:52:15,615 - mmdet - INFO - Epoch [2][250/414]\tlr: 2.000e-02, eta: 0:04:55, time: 0.303, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.2012, loss_rpn_bbox: 0.1496, loss_cls: 0.3241, acc: 86.2109, loss_bbox: 0.2160, loss: 0.8910\n",
      "2022-04-21 15:52:30,617 - mmdet - INFO - Epoch [2][300/414]\tlr: 2.000e-02, eta: 0:04:40, time: 0.300, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1798, loss_rpn_bbox: 0.1532, loss_cls: 0.3063, acc: 87.1855, loss_bbox: 0.2083, loss: 0.8477\n",
      "2022-04-21 15:52:45,349 - mmdet - INFO - Epoch [2][350/414]\tlr: 2.000e-02, eta: 0:04:25, time: 0.295, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1850, loss_rpn_bbox: 0.1529, loss_cls: 0.3108, acc: 86.7930, loss_bbox: 0.2046, loss: 0.8533\n",
      "2022-04-21 15:53:00,099 - mmdet - INFO - Epoch [2][400/414]\tlr: 2.000e-02, eta: 0:04:10, time: 0.295, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1822, loss_rpn_bbox: 0.1449, loss_cls: 0.3052, acc: 86.9668, loss_bbox: 0.2136, loss: 0.8458\n",
      "2022-04-21 15:53:04,150 - mmdet - INFO - Saving checkpoint at 2 epochs\n",
      "2022-04-21 15:53:22,836 - mmdet - INFO - Epoch [3][50/414]\tlr: 2.000e-02, eta: 0:03:49, time: 0.346, data_time: 0.051, memory: 2944, loss_rpn_cls: 0.1837, loss_rpn_bbox: 0.1494, loss_cls: 0.3015, acc: 87.2793, loss_bbox: 0.2026, loss: 0.8373\n",
      "2022-04-21 15:53:37,952 - mmdet - INFO - Epoch [3][100/414]\tlr: 2.000e-02, eta: 0:03:35, time: 0.302, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1865, loss_rpn_bbox: 0.1460, loss_cls: 0.2944, acc: 87.7305, loss_bbox: 0.1986, loss: 0.8256\n",
      "2022-04-21 15:53:53,212 - mmdet - INFO - Epoch [3][150/414]\tlr: 2.000e-02, eta: 0:03:20, time: 0.305, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1555, loss_rpn_bbox: 0.1439, loss_cls: 0.2874, acc: 87.7617, loss_bbox: 0.2138, loss: 0.8006\n",
      "2022-04-21 15:54:08,356 - mmdet - INFO - Epoch [3][200/414]\tlr: 2.000e-02, eta: 0:03:06, time: 0.303, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1657, loss_rpn_bbox: 0.1393, loss_cls: 0.2946, acc: 87.4512, loss_bbox: 0.2228, loss: 0.8224\n",
      "2022-04-21 15:54:23,565 - mmdet - INFO - Epoch [3][250/414]\tlr: 2.000e-02, eta: 0:02:51, time: 0.304, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1521, loss_rpn_bbox: 0.1407, loss_cls: 0.2838, acc: 88.0469, loss_bbox: 0.2268, loss: 0.8035\n",
      "2022-04-21 15:54:38,588 - mmdet - INFO - Epoch [3][300/414]\tlr: 2.000e-02, eta: 0:02:36, time: 0.300, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1487, loss_rpn_bbox: 0.1359, loss_cls: 0.2859, acc: 87.4648, loss_bbox: 0.2193, loss: 0.7899\n",
      "2022-04-21 15:54:53,281 - mmdet - INFO - Epoch [3][350/414]\tlr: 2.000e-02, eta: 0:02:21, time: 0.294, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.1425, loss_rpn_bbox: 0.1334, loss_cls: 0.2938, acc: 87.3516, loss_bbox: 0.2264, loss: 0.7961\n",
      "2022-04-21 15:55:08,207 - mmdet - INFO - Epoch [3][400/414]\tlr: 2.000e-02, eta: 0:02:07, time: 0.299, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1443, loss_rpn_bbox: 0.1284, loss_cls: 0.2915, acc: 87.5273, loss_bbox: 0.2205, loss: 0.7847\n",
      "2022-04-21 15:55:12,150 - mmdet - INFO - Saving checkpoint at 3 epochs\n",
      "2022-04-21 15:55:30,415 - mmdet - INFO - Epoch [4][50/414]\tlr: 2.000e-03, eta: 0:01:47, time: 0.338, data_time: 0.051, memory: 2944, loss_rpn_cls: 0.1355, loss_rpn_bbox: 0.1191, loss_cls: 0.2552, acc: 89.0332, loss_bbox: 0.2065, loss: 0.7162\n",
      "2022-04-21 15:55:45,197 - mmdet - INFO - Epoch [4][100/414]\tlr: 2.000e-03, eta: 0:01:32, time: 0.296, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1178, loss_rpn_bbox: 0.1132, loss_cls: 0.2611, acc: 88.7246, loss_bbox: 0.2121, loss: 0.7042\n",
      "2022-04-21 15:56:00,330 - mmdet - INFO - Epoch [4][150/414]\tlr: 2.000e-03, eta: 0:01:18, time: 0.303, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1125, loss_rpn_bbox: 0.1149, loss_cls: 0.2567, acc: 89.1367, loss_bbox: 0.2155, loss: 0.6996\n",
      "2022-04-21 15:56:15,426 - mmdet - INFO - Epoch [4][200/414]\tlr: 2.000e-03, eta: 0:01:03, time: 0.302, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1101, loss_rpn_bbox: 0.1048, loss_cls: 0.2412, acc: 89.8262, loss_bbox: 0.2062, loss: 0.6623\n",
      "2022-04-21 15:56:30,662 - mmdet - INFO - Epoch [4][250/414]\tlr: 2.000e-03, eta: 0:00:48, time: 0.305, data_time: 0.009, memory: 2944, loss_rpn_cls: 0.1047, loss_rpn_bbox: 0.1020, loss_cls: 0.2451, acc: 89.5957, loss_bbox: 0.2046, loss: 0.6565\n",
      "2022-04-21 15:56:45,354 - mmdet - INFO - Epoch [4][300/414]\tlr: 2.000e-03, eta: 0:00:33, time: 0.294, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.1146, loss_rpn_bbox: 0.1056, loss_cls: 0.2440, acc: 89.6289, loss_bbox: 0.2096, loss: 0.6737\n",
      "2022-04-21 15:57:00,192 - mmdet - INFO - Epoch [4][350/414]\tlr: 2.000e-03, eta: 0:00:18, time: 0.297, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1032, loss_rpn_bbox: 0.1033, loss_cls: 0.2403, acc: 89.9844, loss_bbox: 0.2090, loss: 0.6558\n",
      "2022-04-21 15:57:15,090 - mmdet - INFO - Epoch [4][400/414]\tlr: 2.000e-03, eta: 0:00:04, time: 0.298, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1076, loss_rpn_bbox: 0.1035, loss_cls: 0.2480, acc: 89.5938, loss_bbox: 0.2056, loss: 0.6648\n",
      "2022-04-21 15:57:19,030 - mmdet - INFO - Saving checkpoint at 4 epochs\n"
     ]
    }
   ],
   "source": [
    "datasets = [build_dataset(cfg.data.train)]\n",
    "model.CLASSES = datasets[0].CLASSES\n",
    "train_model(model, datasets, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5232ef9-878b-4e48-819c-c1d832186369",
   "metadata": {},
   "source": [
    "## 训练多目标追踪模型的 ReID 重识别算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cf45e1b-e09d-44c6-ad7b-41fcbfbd4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "cfg = mmcv.Config.fromfile('./configs/reid/resnet50_b32x8_MOT17.py')\n",
    "cfg.data_root = 'data/MOT17_tiny/'\n",
    "cfg.data.test.ann_file = cfg.data.test.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.ann_file = 'data/MOT17_tiny/reid/meta/train_9.txt'\n",
    "cfg.data.val.ann_file = cfg.data.val.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "cfg.data.test.data_prefix = cfg.data.test.data_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.data_prefix = cfg.data.train.data_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.val.data_prefix = cfg.data.val.data_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "# learning policy\n",
    "cfg.lr_config = dict(\n",
    "    policy='step',\n",
    "    warmup='linear',\n",
    "    warmup_iters=200,\n",
    "    warmup_ratio=1.0 / 200,\n",
    "    step=[1])\n",
    "cfg.total_epochs = 2\n",
    "\n",
    "cfg.work_dir = './tutorial_exps/reid'\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f9eede5-8f5d-46f8-9ad9-b9738bb71266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_type = 'ReIDDataset'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadMultiImagesFromFile', to_float32=True),\n",
      "    dict(\n",
      "        type='SeqResize',\n",
      "        img_scale=(128, 256),\n",
      "        share_params=False,\n",
      "        keep_ratio=False,\n",
      "        bbox_clip_border=False,\n",
      "        override=False),\n",
      "    dict(\n",
      "        type='SeqRandomFlip',\n",
      "        share_params=False,\n",
      "        flip_ratio=0.5,\n",
      "        direction='horizontal'),\n",
      "    dict(\n",
      "        type='SeqNormalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='VideoCollect', keys=['img', 'gt_label']),\n",
      "    dict(type='ReIDFormatBundle')\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='Resize', img_scale=(128, 256), keep_ratio=False),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='ImageToTensor', keys=['img']),\n",
      "    dict(type='Collect', keys=['img'], meta_keys=[])\n",
      "]\n",
      "data_root = 'data/MOT17_tiny/'\n",
      "data = dict(\n",
      "    samples_per_gpu=1,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='ReIDDataset',\n",
      "        triplet_sampler=dict(num_ids=8, ins_per_id=4),\n",
      "        data_prefix='data/MOT17_tiny/reid/imgs',\n",
      "        ann_file='data/MOT17_tiny/reid/meta/train_9.txt',\n",
      "        pipeline=[\n",
      "            dict(type='LoadMultiImagesFromFile', to_float32=True),\n",
      "            dict(\n",
      "                type='SeqResize',\n",
      "                img_scale=(128, 256),\n",
      "                share_params=False,\n",
      "                keep_ratio=False,\n",
      "                bbox_clip_border=False,\n",
      "                override=False),\n",
      "            dict(\n",
      "                type='SeqRandomFlip',\n",
      "                share_params=False,\n",
      "                flip_ratio=0.5,\n",
      "                direction='horizontal'),\n",
      "            dict(\n",
      "                type='SeqNormalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='VideoCollect', keys=['img', 'gt_label']),\n",
      "            dict(type='ReIDFormatBundle')\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='ReIDDataset',\n",
      "        triplet_sampler=None,\n",
      "        data_prefix='data/MOT17_tiny/reid/imgs',\n",
      "        ann_file='data/MOT17_tiny/reid/meta/val_20.txt',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='Resize', img_scale=(128, 256), keep_ratio=False),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'], meta_keys=[])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='ReIDDataset',\n",
      "        triplet_sampler=None,\n",
      "        data_prefix='data/MOT17_tiny/reid/imgs',\n",
      "        ann_file='data/MOT17_tiny/reid/meta/val_20.txt',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='Resize', img_scale=(128, 256), keep_ratio=False),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'], meta_keys=[])\n",
      "        ]))\n",
      "evaluation = dict(interval=1, metric='mAP')\n",
      "optimizer = dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "TRAIN_REID = True\n",
      "model = dict(\n",
      "    reid=dict(\n",
      "        type='BaseReID',\n",
      "        backbone=dict(\n",
      "            type='ResNet',\n",
      "            depth=50,\n",
      "            num_stages=4,\n",
      "            out_indices=(3, ),\n",
      "            style='pytorch'),\n",
      "        neck=dict(type='GlobalAveragePooling', kernel_size=(8, 4), stride=1),\n",
      "        head=dict(\n",
      "            type='LinearReIDHead',\n",
      "            num_fcs=1,\n",
      "            in_channels=2048,\n",
      "            fc_channels=1024,\n",
      "            out_channels=128,\n",
      "            num_classes=380,\n",
      "            loss=dict(type='CrossEntropyLoss', loss_weight=1.0),\n",
      "            loss_pairwise=dict(\n",
      "                type='TripletLoss', margin=0.3, loss_weight=1.0),\n",
      "            norm_cfg=dict(type='BN1d'),\n",
      "            act_cfg=dict(type='ReLU')),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint=\n",
      "            'https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_batch256_imagenet_20200708-cfb998bf.pth'\n",
      "        )))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=200,\n",
      "    warmup_ratio=0.005,\n",
      "    step=[1])\n",
      "total_epochs = 2\n",
      "work_dir = './tutorial_exps/reid'\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a268637-fa50-4b99-a531-cdaa9054f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 16:17:56,819 - mmcv - INFO - initialize BaseReID with init_cfg {'type': 'Pretrained', 'checkpoint': 'https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_batch256_imagenet_20200708-cfb998bf.pth'}\n",
      "2022-04-21 16:17:56,821 - mmcv - INFO - load model from: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_batch256_imagenet_20200708-cfb998bf.pth\n",
      "2022-04-21 16:17:56,821 - mmcv - INFO - load checkpoint from http path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_batch256_imagenet_20200708-cfb998bf.pth\n",
      "Downloading: \"https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_batch256_imagenet_20200708-cfb998bf.pth\" to /home/featurize/.cache/torch/hub/checkpoints/resnet50_batch256_imagenet_20200708-cfb998bf.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446c86f2a1d445ac876f8302a4fbad61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 16:17:59,037 - mmcv - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: head.fc.weight, head.fc.bias\n",
      "\n",
      "missing keys in source state_dict: head.fcs.0.fc.weight, head.fcs.0.fc.bias, head.fcs.0.bn.weight, head.fcs.0.bn.bias, head.fcs.0.bn.running_mean, head.fcs.0.bn.running_var, head.fc_out.weight, head.fc_out.bias, head.bn.weight, head.bn.bias, head.bn.running_mean, head.bn.running_var, head.classifier.weight, head.classifier.bias\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmtrack.datasets import build_dataset\n",
    "from mmdet.apis import train_detector as train_model\n",
    "from mmtrack.models import build_reid as build_model\n",
    "\n",
    "\n",
    "model = build_model(cfg.model.reid)\n",
    "model.init_weights()\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "model.CLASSES = datasets[0].CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "782f64d1-ae99-46b7-89c7-21a6e06f76d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 16:18:01,934 - mmdet - INFO - Start running, host: featurize@featurize, work_dir: /home/featurize/work/MMTracking教程/0420/mmtracking/tutorial_exps/reid\n",
      "2022-04-21 16:18:01,936 - mmdet - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-04-21 16:18:01,937 - mmdet - INFO - workflow: [('train', 1)], max: 2 epochs\n",
      "2022-04-21 16:18:01,938 - mmdet - INFO - Checkpoints will be saved to /home/featurize/work/MMTracking教程/0420/mmtracking/tutorial_exps/reid by HardDiskBackend.\n",
      "2022-04-21 16:18:11,777 - mmdet - INFO - Epoch [1][50/1576]\tlr: 2.488e-02, eta: 0:10:07, time: 0.196, data_time: 0.049, memory: 2944, triplet_loss: 0.0963, ce_loss: 0.8359, top-1: 90.8750, loss: 0.9322\n",
      "2022-04-21 16:18:19,240 - mmdet - INFO - Epoch [1][100/1576]\tlr: 4.975e-02, eta: 0:08:46, time: 0.149, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0003, top-1: 100.0000, loss: 0.0003\n",
      "2022-04-21 16:18:26,632 - mmdet - INFO - Epoch [1][150/1576]\tlr: 7.463e-02, eta: 0:08:13, time: 0.148, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:18:33,990 - mmdet - INFO - Epoch [1][200/1576]\tlr: 9.950e-02, eta: 0:07:52, time: 0.147, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:18:41,279 - mmdet - INFO - Epoch [1][250/1576]\tlr: 1.000e-01, eta: 0:07:36, time: 0.146, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:18:48,582 - mmdet - INFO - Epoch [1][300/1576]\tlr: 1.000e-01, eta: 0:07:22, time: 0.146, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:18:55,936 - mmdet - INFO - Epoch [1][350/1576]\tlr: 1.000e-01, eta: 0:07:11, time: 0.147, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:19:03,265 - mmdet - INFO - Epoch [1][400/1576]\tlr: 1.000e-01, eta: 0:07:01, time: 0.147, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:19:10,626 - mmdet - INFO - Epoch [1][450/1576]\tlr: 1.000e-01, eta: 0:06:52, time: 0.147, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:19:18,020 - mmdet - INFO - Epoch [1][500/1576]\tlr: 1.000e-01, eta: 0:06:43, time: 0.148, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:19:25,396 - mmdet - INFO - Epoch [1][550/1576]\tlr: 1.000e-01, eta: 0:06:34, time: 0.148, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:19:32,771 - mmdet - INFO - Epoch [1][600/1576]\tlr: 1.000e-01, eta: 0:06:26, time: 0.147, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:19:40,179 - mmdet - INFO - Epoch [1][650/1576]\tlr: 1.000e-01, eta: 0:06:17, time: 0.148, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:19:47,695 - mmdet - INFO - Epoch [1][700/1576]\tlr: 1.000e-01, eta: 0:06:10, time: 0.150, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:19:55,306 - mmdet - INFO - Epoch [1][750/1576]\tlr: 1.000e-01, eta: 0:06:02, time: 0.152, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:20:02,903 - mmdet - INFO - Epoch [1][800/1576]\tlr: 1.000e-01, eta: 0:05:55, time: 0.152, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:20:10,526 - mmdet - INFO - Epoch [1][850/1576]\tlr: 1.000e-01, eta: 0:05:48, time: 0.153, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:20:18,140 - mmdet - INFO - Epoch [1][900/1576]\tlr: 1.000e-01, eta: 0:05:40, time: 0.152, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:20:25,629 - mmdet - INFO - Epoch [1][950/1576]\tlr: 1.000e-01, eta: 0:05:32, time: 0.150, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:20:33,097 - mmdet - INFO - Epoch [1][1000/1576]\tlr: 1.000e-01, eta: 0:05:25, time: 0.149, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:20:40,552 - mmdet - INFO - Epoch [1][1050/1576]\tlr: 1.000e-01, eta: 0:05:17, time: 0.149, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-21 16:20:47,918 - mmdet - INFO - Epoch [1][1100/1576]\tlr: 1.000e-01, eta: 0:05:09, time: 0.147, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:20:55,571 - mmdet - INFO - Epoch [1][1150/1576]\tlr: 1.000e-01, eta: 0:05:02, time: 0.153, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:21:03,299 - mmdet - INFO - Epoch [1][1200/1576]\tlr: 1.000e-01, eta: 0:04:54, time: 0.155, data_time: 0.006, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:21:10,965 - mmdet - INFO - Epoch [1][1250/1576]\tlr: 1.000e-01, eta: 0:04:47, time: 0.153, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:21:18,700 - mmdet - INFO - Epoch [1][1300/1576]\tlr: 1.000e-01, eta: 0:04:40, time: 0.155, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:21:26,288 - mmdet - INFO - Epoch [1][1350/1576]\tlr: 1.000e-01, eta: 0:04:32, time: 0.152, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:21:33,938 - mmdet - INFO - Epoch [1][1400/1576]\tlr: 1.000e-01, eta: 0:04:25, time: 0.153, data_time: 0.006, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:21:41,567 - mmdet - INFO - Epoch [1][1450/1576]\tlr: 1.000e-01, eta: 0:04:17, time: 0.153, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:21:49,277 - mmdet - INFO - Epoch [1][1500/1576]\tlr: 1.000e-01, eta: 0:04:10, time: 0.154, data_time: 0.006, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:21:56,908 - mmdet - INFO - Epoch [1][1550/1576]\tlr: 1.000e-01, eta: 0:04:02, time: 0.153, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:22:00,855 - mmdet - INFO - Saving checkpoint at 1 epochs\n",
      "2022-04-21 16:22:11,535 - mmdet - INFO - Epoch [2][50/1576]\tlr: 1.000e-02, eta: 0:03:49, time: 0.198, data_time: 0.049, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:22:19,235 - mmdet - INFO - Epoch [2][100/1576]\tlr: 1.000e-02, eta: 0:03:42, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:22:26,683 - mmdet - INFO - Epoch [2][150/1576]\tlr: 1.000e-02, eta: 0:03:34, time: 0.149, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:22:34,110 - mmdet - INFO - Epoch [2][200/1576]\tlr: 1.000e-02, eta: 0:03:27, time: 0.149, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:22:41,528 - mmdet - INFO - Epoch [2][250/1576]\tlr: 1.000e-02, eta: 0:03:19, time: 0.148, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:22:49,205 - mmdet - INFO - Epoch [2][300/1576]\tlr: 1.000e-02, eta: 0:03:12, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:22:56,907 - mmdet - INFO - Epoch [2][350/1576]\tlr: 1.000e-02, eta: 0:03:04, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:23:04,585 - mmdet - INFO - Epoch [2][400/1576]\tlr: 1.000e-02, eta: 0:02:57, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:23:12,213 - mmdet - INFO - Epoch [2][450/1576]\tlr: 1.000e-02, eta: 0:02:49, time: 0.153, data_time: 0.006, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:23:19,776 - mmdet - INFO - Epoch [2][500/1576]\tlr: 1.000e-02, eta: 0:02:42, time: 0.151, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:23:27,324 - mmdet - INFO - Epoch [2][550/1576]\tlr: 1.000e-02, eta: 0:02:34, time: 0.151, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:23:34,775 - mmdet - INFO - Epoch [2][600/1576]\tlr: 1.000e-02, eta: 0:02:27, time: 0.149, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:23:42,265 - mmdet - INFO - Epoch [2][650/1576]\tlr: 1.000e-02, eta: 0:02:19, time: 0.150, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:23:49,773 - mmdet - INFO - Epoch [2][700/1576]\tlr: 1.000e-02, eta: 0:02:12, time: 0.150, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:23:57,302 - mmdet - INFO - Epoch [2][750/1576]\tlr: 1.000e-02, eta: 0:02:04, time: 0.151, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:24:04,824 - mmdet - INFO - Epoch [2][800/1576]\tlr: 1.000e-02, eta: 0:01:56, time: 0.150, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:24:12,325 - mmdet - INFO - Epoch [2][850/1576]\tlr: 1.000e-02, eta: 0:01:49, time: 0.150, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:24:20,020 - mmdet - INFO - Epoch [2][900/1576]\tlr: 1.000e-02, eta: 0:01:41, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:24:27,727 - mmdet - INFO - Epoch [2][950/1576]\tlr: 1.000e-02, eta: 0:01:34, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:24:35,346 - mmdet - INFO - Epoch [2][1000/1576]\tlr: 1.000e-02, eta: 0:01:26, time: 0.152, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:24:42,847 - mmdet - INFO - Epoch [2][1050/1576]\tlr: 1.000e-02, eta: 0:01:19, time: 0.150, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:24:50,277 - mmdet - INFO - Epoch [2][1100/1576]\tlr: 1.000e-02, eta: 0:01:11, time: 0.149, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:24:57,754 - mmdet - INFO - Epoch [2][1150/1576]\tlr: 1.000e-02, eta: 0:01:04, time: 0.150, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:25:05,145 - mmdet - INFO - Epoch [2][1200/1576]\tlr: 1.000e-02, eta: 0:00:56, time: 0.148, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:25:12,529 - mmdet - INFO - Epoch [2][1250/1576]\tlr: 1.000e-02, eta: 0:00:49, time: 0.148, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:25:19,921 - mmdet - INFO - Epoch [2][1300/1576]\tlr: 1.000e-02, eta: 0:00:41, time: 0.148, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:25:27,263 - mmdet - INFO - Epoch [2][1350/1576]\tlr: 1.000e-02, eta: 0:00:34, time: 0.147, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:25:34,857 - mmdet - INFO - Epoch [2][1400/1576]\tlr: 1.000e-02, eta: 0:00:26, time: 0.152, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:25:42,474 - mmdet - INFO - Epoch [2][1450/1576]\tlr: 1.000e-02, eta: 0:00:18, time: 0.152, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:25:49,998 - mmdet - INFO - Epoch [2][1500/1576]\tlr: 1.000e-02, eta: 0:00:11, time: 0.151, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:25:57,677 - mmdet - INFO - Epoch [2][1550/1576]\tlr: 1.000e-02, eta: 0:00:03, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0002, top-1: 100.0000, loss: 0.0002\n",
      "2022-04-21 16:26:01,649 - mmdet - INFO - Saving checkpoint at 2 epochs\n"
     ]
    }
   ],
   "source": [
    "train_model(model, datasets, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08fb29-da9b-48e7-89d1-eb2b6a67e4b3",
   "metadata": {},
   "source": [
    "## 测试DeepSORT多目标追踪算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa417e28-95d7-4777-b196-f42e285b1b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    detector=dict(\n",
      "        type='FasterRCNN',\n",
      "        backbone=dict(\n",
      "            type='ResNet',\n",
      "            depth=50,\n",
      "            num_stages=4,\n",
      "            out_indices=(0, 1, 2, 3),\n",
      "            frozen_stages=1,\n",
      "            norm_cfg=dict(type='BN', requires_grad=True),\n",
      "            norm_eval=True,\n",
      "            style='pytorch',\n",
      "            init_cfg=dict(\n",
      "                type='Pretrained', checkpoint='torchvision://resnet50')),\n",
      "        neck=dict(\n",
      "            type='FPN',\n",
      "            in_channels=[256, 512, 1024, 2048],\n",
      "            out_channels=256,\n",
      "            num_outs=5),\n",
      "        rpn_head=dict(\n",
      "            type='RPNHead',\n",
      "            in_channels=256,\n",
      "            feat_channels=256,\n",
      "            anchor_generator=dict(\n",
      "                type='AnchorGenerator',\n",
      "                scales=[8],\n",
      "                ratios=[0.5, 1.0, 2.0],\n",
      "                strides=[4, 8, 16, 32, 64]),\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[1.0, 1.0, 1.0, 1.0],\n",
      "                clip_border=False),\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "            loss_bbox=dict(\n",
      "                type='SmoothL1Loss', beta=0.1111111111111111,\n",
      "                loss_weight=1.0)),\n",
      "        roi_head=dict(\n",
      "            type='StandardRoIHead',\n",
      "            bbox_roi_extractor=dict(\n",
      "                type='SingleRoIExtractor',\n",
      "                roi_layer=dict(\n",
      "                    type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "                out_channels=256,\n",
      "                featmap_strides=[4, 8, 16, 32]),\n",
      "            bbox_head=dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=1,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.1, 0.1, 0.2, 0.2],\n",
      "                    clip_border=False),\n",
      "                reg_class_agnostic=False,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', loss_weight=1.0))),\n",
      "        train_cfg=dict(\n",
      "            rpn=dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.7,\n",
      "                    neg_iou_thr=0.3,\n",
      "                    min_pos_iou=0.3,\n",
      "                    match_low_quality=True,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=256,\n",
      "                    pos_fraction=0.5,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=False),\n",
      "                allowed_border=-1,\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            rpn_proposal=dict(\n",
      "                nms_pre=2000,\n",
      "                max_per_img=1000,\n",
      "                nms=dict(type='nms', iou_threshold=0.7),\n",
      "                min_bbox_size=0),\n",
      "            rcnn=dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    min_pos_iou=0.5,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False)),\n",
      "        test_cfg=dict(\n",
      "            rpn=dict(\n",
      "                nms_pre=1000,\n",
      "                max_per_img=1000,\n",
      "                nms=dict(type='nms', iou_threshold=0.7),\n",
      "                min_bbox_size=0),\n",
      "            rcnn=dict(\n",
      "                score_thr=0.05,\n",
      "                nms=dict(type='nms', iou_threshold=0.5),\n",
      "                max_per_img=100)),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint='./tutorial_exps/detector/epoch_4.pth')),\n",
      "    type='DeepSORT',\n",
      "    motion=dict(type='KalmanFilter', center_only=False),\n",
      "    reid=dict(\n",
      "        type='BaseReID',\n",
      "        backbone=dict(\n",
      "            type='ResNet',\n",
      "            depth=50,\n",
      "            num_stages=4,\n",
      "            out_indices=(3, ),\n",
      "            style='pytorch'),\n",
      "        neck=dict(type='GlobalAveragePooling', kernel_size=(8, 4), stride=1),\n",
      "        head=dict(\n",
      "            type='LinearReIDHead',\n",
      "            num_fcs=1,\n",
      "            in_channels=2048,\n",
      "            fc_channels=1024,\n",
      "            out_channels=128,\n",
      "            num_classes=380,\n",
      "            loss=dict(type='CrossEntropyLoss', loss_weight=1.0),\n",
      "            loss_pairwise=dict(\n",
      "                type='TripletLoss', margin=0.3, loss_weight=1.0),\n",
      "            norm_cfg=dict(type='BN1d'),\n",
      "            act_cfg=dict(type='ReLU')),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained', checkpoint='./tutorial_exps/reid/epoch_2.pth')),\n",
      "    tracker=dict(\n",
      "        type='SortTracker',\n",
      "        obj_score_thr=0.5,\n",
      "        reid=dict(\n",
      "            num_samples=10,\n",
      "            img_scale=(256, 128),\n",
      "            img_norm_cfg=None,\n",
      "            match_score_thr=2.0),\n",
      "        match_iou_thr=0.5,\n",
      "        momentums=None,\n",
      "        num_tentatives=2,\n",
      "        num_frames_retain=100))\n",
      "dataset_type = 'MOTChallengeDataset'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadMultiImagesFromFile', to_float32=True),\n",
      "    dict(type='SeqLoadAnnotations', with_bbox=True, with_track=True),\n",
      "    dict(\n",
      "        type='SeqResize',\n",
      "        img_scale=(1088, 1088),\n",
      "        share_params=True,\n",
      "        ratio_range=(0.8, 1.2),\n",
      "        keep_ratio=True,\n",
      "        bbox_clip_border=False),\n",
      "    dict(type='SeqPhotoMetricDistortion', share_params=True),\n",
      "    dict(\n",
      "        type='SeqRandomCrop',\n",
      "        share_params=False,\n",
      "        crop_size=(1088, 1088),\n",
      "        bbox_clip_border=False),\n",
      "    dict(type='SeqRandomFlip', share_params=True, flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='SeqNormalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='SeqPad', size_divisor=32),\n",
      "    dict(type='MatchInstances', skip_nomatch=True),\n",
      "    dict(\n",
      "        type='VideoCollect',\n",
      "        keys=[\n",
      "            'img', 'gt_bboxes', 'gt_labels', 'gt_match_indices',\n",
      "            'gt_instance_ids'\n",
      "        ]),\n",
      "    dict(type='SeqDefaultFormatBundle', ref_prefix='ref')\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1088, 1088),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='VideoCollect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data_root = 'data/MOT17_tiny/'\n",
      "data = dict(\n",
      "    samples_per_gpu=2,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='MOTChallengeDataset',\n",
      "        visibility_thr=-1,\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        ref_img_sampler=dict(\n",
      "            num_ref_imgs=1,\n",
      "            frame_range=10,\n",
      "            filter_key_img=True,\n",
      "            method='uniform'),\n",
      "        pipeline=[\n",
      "            dict(type='LoadMultiImagesFromFile', to_float32=True),\n",
      "            dict(type='SeqLoadAnnotations', with_bbox=True, with_track=True),\n",
      "            dict(\n",
      "                type='SeqResize',\n",
      "                img_scale=(1088, 1088),\n",
      "                share_params=True,\n",
      "                ratio_range=(0.8, 1.2),\n",
      "                keep_ratio=True,\n",
      "                bbox_clip_border=False),\n",
      "            dict(type='SeqPhotoMetricDistortion', share_params=True),\n",
      "            dict(\n",
      "                type='SeqRandomCrop',\n",
      "                share_params=False,\n",
      "                crop_size=(1088, 1088),\n",
      "                bbox_clip_border=False),\n",
      "            dict(type='SeqRandomFlip', share_params=True, flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='SeqNormalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='SeqPad', size_divisor=32),\n",
      "            dict(type='MatchInstances', skip_nomatch=True),\n",
      "            dict(\n",
      "                type='VideoCollect',\n",
      "                keys=[\n",
      "                    'img', 'gt_bboxes', 'gt_labels', 'gt_match_indices',\n",
      "                    'gt_instance_ids'\n",
      "                ]),\n",
      "            dict(type='SeqDefaultFormatBundle', ref_prefix='ref')\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='MOTChallengeDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        ref_img_sampler=None,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1088, 1088),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='VideoCollect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='MOTChallengeDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        ref_img_sampler=None,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1088, 1088),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='VideoCollect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        test_mode=True))\n",
      "optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=100,\n",
      "    warmup_ratio=0.01,\n",
      "    step=[3])\n",
      "total_epochs = 4\n",
      "evaluation = dict(metric=['bbox', 'track'], interval=1)\n",
      "search_metrics = ['MOTA', 'IDF1', 'FN', 'FP', 'IDs', 'MT', 'ML']\n",
      "work_dir = './tutorial_exps'\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mmcv\n",
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "cfg = mmcv.Config.fromfile('./configs/mot/deepsort/deepsort_faster-rcnn_fpn_4e_mot17-private-half.py')\n",
    "cfg.data_root = 'data/MOT17_tiny/'\n",
    "cfg.data.test.ann_file = cfg.data.test.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.ann_file = cfg.data.test.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.val.ann_file = cfg.data.val.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "cfg.data.test.img_prefix = cfg.data.test.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.img_prefix = cfg.data.train.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.val.img_prefix = cfg.data.val.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "cfg.model.detector.init_cfg.checkpoint = './tutorial_exps/detector/epoch_4.pth'\n",
    "cfg.model.reid.init_cfg.checkpoint = './tutorial_exps/reid/epoch_2.pth'\n",
    "\n",
    "cfg.work_dir = './tutorial_exps'\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "cfg.data.test.test_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9431013c-d2b6-4d97-9272-0c6914e99ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = dict(\n",
      "    detector=dict(\n",
      "        type='FasterRCNN',\n",
      "        backbone=dict(\n",
      "            type='ResNet',\n",
      "            depth=50,\n",
      "            num_stages=4,\n",
      "            out_indices=(0, 1, 2, 3),\n",
      "            frozen_stages=1,\n",
      "            norm_cfg=dict(type='BN', requires_grad=True),\n",
      "            norm_eval=True,\n",
      "            style='pytorch',\n",
      "            init_cfg=dict(\n",
      "                type='Pretrained', checkpoint='torchvision://resnet50')),\n",
      "        neck=dict(\n",
      "            type='FPN',\n",
      "            in_channels=[256, 512, 1024, 2048],\n",
      "            out_channels=256,\n",
      "            num_outs=5),\n",
      "        rpn_head=dict(\n",
      "            type='RPNHead',\n",
      "            in_channels=256,\n",
      "            feat_channels=256,\n",
      "            anchor_generator=dict(\n",
      "                type='AnchorGenerator',\n",
      "                scales=[8],\n",
      "                ratios=[0.5, 1.0, 2.0],\n",
      "                strides=[4, 8, 16, 32, 64]),\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[1.0, 1.0, 1.0, 1.0],\n",
      "                clip_border=False),\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "            loss_bbox=dict(\n",
      "                type='SmoothL1Loss', beta=0.1111111111111111,\n",
      "                loss_weight=1.0)),\n",
      "        roi_head=dict(\n",
      "            type='StandardRoIHead',\n",
      "            bbox_roi_extractor=dict(\n",
      "                type='SingleRoIExtractor',\n",
      "                roi_layer=dict(\n",
      "                    type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "                out_channels=256,\n",
      "                featmap_strides=[4, 8, 16, 32]),\n",
      "            bbox_head=dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=1,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.1, 0.1, 0.2, 0.2],\n",
      "                    clip_border=False),\n",
      "                reg_class_agnostic=False,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', loss_weight=1.0))),\n",
      "        train_cfg=dict(\n",
      "            rpn=dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.7,\n",
      "                    neg_iou_thr=0.3,\n",
      "                    min_pos_iou=0.3,\n",
      "                    match_low_quality=True,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=256,\n",
      "                    pos_fraction=0.5,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=False),\n",
      "                allowed_border=-1,\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            rpn_proposal=dict(\n",
      "                nms_pre=2000,\n",
      "                max_per_img=1000,\n",
      "                nms=dict(type='nms', iou_threshold=0.7),\n",
      "                min_bbox_size=0),\n",
      "            rcnn=dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    min_pos_iou=0.5,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False)),\n",
      "        test_cfg=dict(\n",
      "            rpn=dict(\n",
      "                nms_pre=1000,\n",
      "                max_per_img=1000,\n",
      "                nms=dict(type='nms', iou_threshold=0.7),\n",
      "                min_bbox_size=0),\n",
      "            rcnn=dict(\n",
      "                score_thr=0.05,\n",
      "                nms=dict(type='nms', iou_threshold=0.5),\n",
      "                max_per_img=100)),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint='./tutorial_exps/detector/epoch_4.pth')),\n",
      "    type='DeepSORT',\n",
      "    motion=dict(type='KalmanFilter', center_only=False),\n",
      "    reid=dict(\n",
      "        type='BaseReID',\n",
      "        backbone=dict(\n",
      "            type='ResNet',\n",
      "            depth=50,\n",
      "            num_stages=4,\n",
      "            out_indices=(3, ),\n",
      "            style='pytorch'),\n",
      "        neck=dict(type='GlobalAveragePooling', kernel_size=(8, 4), stride=1),\n",
      "        head=dict(\n",
      "            type='LinearReIDHead',\n",
      "            num_fcs=1,\n",
      "            in_channels=2048,\n",
      "            fc_channels=1024,\n",
      "            out_channels=128,\n",
      "            num_classes=380,\n",
      "            loss=dict(type='CrossEntropyLoss', loss_weight=1.0),\n",
      "            loss_pairwise=dict(\n",
      "                type='TripletLoss', margin=0.3, loss_weight=1.0),\n",
      "            norm_cfg=dict(type='BN1d'),\n",
      "            act_cfg=dict(type='ReLU')),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained', checkpoint='./tutorial_exps/reid/epoch_2.pth')),\n",
      "    tracker=dict(\n",
      "        type='SortTracker',\n",
      "        obj_score_thr=0.5,\n",
      "        reid=dict(\n",
      "            num_samples=10,\n",
      "            img_scale=(256, 128),\n",
      "            img_norm_cfg=None,\n",
      "            match_score_thr=2.0),\n",
      "        match_iou_thr=0.5,\n",
      "        momentums=None,\n",
      "        num_tentatives=2,\n",
      "        num_frames_retain=100))\n",
      "dataset_type = 'MOTChallengeDataset'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadMultiImagesFromFile', to_float32=True),\n",
      "    dict(type='SeqLoadAnnotations', with_bbox=True, with_track=True),\n",
      "    dict(\n",
      "        type='SeqResize',\n",
      "        img_scale=(1088, 1088),\n",
      "        share_params=True,\n",
      "        ratio_range=(0.8, 1.2),\n",
      "        keep_ratio=True,\n",
      "        bbox_clip_border=False),\n",
      "    dict(type='SeqPhotoMetricDistortion', share_params=True),\n",
      "    dict(\n",
      "        type='SeqRandomCrop',\n",
      "        share_params=False,\n",
      "        crop_size=(1088, 1088),\n",
      "        bbox_clip_border=False),\n",
      "    dict(type='SeqRandomFlip', share_params=True, flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='SeqNormalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='SeqPad', size_divisor=32),\n",
      "    dict(type='MatchInstances', skip_nomatch=True),\n",
      "    dict(\n",
      "        type='VideoCollect',\n",
      "        keys=[\n",
      "            'img', 'gt_bboxes', 'gt_labels', 'gt_match_indices',\n",
      "            'gt_instance_ids'\n",
      "        ]),\n",
      "    dict(type='SeqDefaultFormatBundle', ref_prefix='ref')\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1088, 1088),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='VideoCollect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data_root = 'data/MOT17_tiny/'\n",
      "data = dict(\n",
      "    samples_per_gpu=2,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='MOTChallengeDataset',\n",
      "        visibility_thr=-1,\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        ref_img_sampler=dict(\n",
      "            num_ref_imgs=1,\n",
      "            frame_range=10,\n",
      "            filter_key_img=True,\n",
      "            method='uniform'),\n",
      "        pipeline=[\n",
      "            dict(type='LoadMultiImagesFromFile', to_float32=True),\n",
      "            dict(type='SeqLoadAnnotations', with_bbox=True, with_track=True),\n",
      "            dict(\n",
      "                type='SeqResize',\n",
      "                img_scale=(1088, 1088),\n",
      "                share_params=True,\n",
      "                ratio_range=(0.8, 1.2),\n",
      "                keep_ratio=True,\n",
      "                bbox_clip_border=False),\n",
      "            dict(type='SeqPhotoMetricDistortion', share_params=True),\n",
      "            dict(\n",
      "                type='SeqRandomCrop',\n",
      "                share_params=False,\n",
      "                crop_size=(1088, 1088),\n",
      "                bbox_clip_border=False),\n",
      "            dict(type='SeqRandomFlip', share_params=True, flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='SeqNormalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='SeqPad', size_divisor=32),\n",
      "            dict(type='MatchInstances', skip_nomatch=True),\n",
      "            dict(\n",
      "                type='VideoCollect',\n",
      "                keys=[\n",
      "                    'img', 'gt_bboxes', 'gt_labels', 'gt_match_indices',\n",
      "                    'gt_instance_ids'\n",
      "                ]),\n",
      "            dict(type='SeqDefaultFormatBundle', ref_prefix='ref')\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='MOTChallengeDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        ref_img_sampler=None,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1088, 1088),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='VideoCollect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='MOTChallengeDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        ref_img_sampler=None,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1088, 1088),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='VideoCollect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        test_mode=True))\n",
      "optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=100,\n",
      "    warmup_ratio=0.01,\n",
      "    step=[3])\n",
      "total_epochs = 4\n",
      "evaluation = dict(metric=['bbox', 'track'], interval=1)\n",
      "search_metrics = ['MOTA', 'IDF1', 'FN', 'FP', 'IDs', 'MT', 'ML']\n",
      "work_dir = './tutorial_exps'\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a50ecdc-f362-4223-9fb4-91b94e3cca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.16s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 16:28:06,681 - mmcv - INFO - initialize FasterRCNN with init_cfg {'type': 'Pretrained', 'checkpoint': './tutorial_exps/detector/epoch_4.pth'}\n",
      "2022-04-21 16:28:06,683 - mmcv - INFO - load model from: ./tutorial_exps/detector/epoch_4.pth\n",
      "2022-04-21 16:28:06,684 - mmcv - INFO - load checkpoint from local path: ./tutorial_exps/detector/epoch_4.pth\n",
      "2022-04-21 16:28:14,816 - mmcv - INFO - initialize BaseReID with init_cfg {'type': 'Pretrained', 'checkpoint': './tutorial_exps/reid/epoch_2.pth'}\n",
      "2022-04-21 16:28:14,817 - mmcv - INFO - load model from: ./tutorial_exps/reid/epoch_2.pth\n",
      "2022-04-21 16:28:14,819 - mmcv - INFO - load checkpoint from local path: ./tutorial_exps/reid/epoch_2.pth\n",
      "2022-04-21 16:28:15,025 - mmcv - INFO - \n",
      "detector.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,026 - mmcv - INFO - \n",
      "detector.backbone.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,026 - mmcv - INFO - \n",
      "detector.backbone.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,028 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,028 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,029 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,029 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,030 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn2.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,031 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn2.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,031 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,032 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn3.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,032 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn3.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,033 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,033 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.downsample.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,034 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.downsample.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,035 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,035 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,036 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,037 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,037 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn2.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,038 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn2.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,038 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,039 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn3.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,040 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn3.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,043 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,044 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,044 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,045 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,045 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn2.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,046 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn2.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,046 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,047 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn3.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,048 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn3.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-21 16:28:15,048 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,049 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,050 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,050 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,051 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,051 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,052 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,052 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,053 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,053 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,054 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.downsample.1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,055 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.downsample.1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,055 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,056 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,056 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,057 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,058 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,058 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,059 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,059 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,060 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,060 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,061 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,062 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,062 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,063 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,063 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,064 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,064 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,065 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,066 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,066 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,067 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,067 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,068 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,068 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,069 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,070 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,070 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,071 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,072 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,072 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,073 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,073 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,074 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,074 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,075 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,076 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,076 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,077 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,077 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,078 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,079 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,079 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,080 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,080 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,081 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,081 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,082 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,083 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,083 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,084 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,084 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,085 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,085 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,086 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,087 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,087 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,088 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,088 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,089 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,090 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,090 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,091 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,092 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,092 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,093 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,093 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,094 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,094 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,095 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,096 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,096 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,097 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,097 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,098 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,098 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,099 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,100 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,100 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,101 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,101 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,102 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,103 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,103 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,111 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,112 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,113 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,113 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,114 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,114 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,115 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,116 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,116 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,118 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,119 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,120 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,120 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,121 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,121 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,122 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,123 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,123 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,124 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,124 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,125 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,125 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,126 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,127 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,127 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,128 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,128 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,131 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,132 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,132 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,133 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,133 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,138 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.0.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,138 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,139 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.1.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,140 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,140 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.2.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,141 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,141 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.3.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,142 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,142 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.0.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,143 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,143 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.1.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,144 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,145 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.2.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,145 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,146 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.3.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,146 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,147 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,148 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_cls.weight - torch.Size([3, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,149 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_cls.bias - torch.Size([3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,149 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_reg.weight - torch.Size([12, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,150 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_reg.bias - torch.Size([12]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,150 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_cls.weight - torch.Size([2, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,151 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_cls.bias - torch.Size([2]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,152 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_reg.weight - torch.Size([4, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,152 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_reg.bias - torch.Size([4]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,153 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,153 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,154 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,155 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-21 16:28:15,155 - mmcv - INFO - \n",
      "reid.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,156 - mmcv - INFO - \n",
      "reid.backbone.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,156 - mmcv - INFO - \n",
      "reid.backbone.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,157 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,157 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,158 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,159 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,159 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,160 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,160 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,161 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,161 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,162 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,162 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.downsample.1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,163 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.downsample.1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,164 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,164 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,165 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,165 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,166 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,167 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,167 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,168 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,168 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,169 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,170 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,170 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,171 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,171 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,172 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,172 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,173 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,174 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,174 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,175 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,175 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,183 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,184 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,184 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,185 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,186 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,186 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,187 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,187 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.downsample.1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,188 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.downsample.1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,189 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,189 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,190 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,190 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,191 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,192 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,193 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,193 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,194 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,195 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,195 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,196 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,197 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,197 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,198 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,200 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,200 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,201 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,201 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,202 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,203 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,203 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,204 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,204 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,205 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,205 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,206 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,207 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,207 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,208 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,208 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,209 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,209 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,210 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,211 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,211 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,212 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,212 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,213 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,213 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,214 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,214 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,215 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,216 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,216 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,217 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,217 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,218 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,218 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,219 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,220 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,220 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,221 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,221 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,222 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,223 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,223 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,224 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,224 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,225 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,225 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,226 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,227 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,227 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,228 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,229 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,229 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,230 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,230 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,231 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,232 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,232 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,233 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,233 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,234 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,234 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,235 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,236 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,236 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,237 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,237 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,238 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,238 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,239 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,240 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,240 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,241 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,241 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,242 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,242 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,243 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,244 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,244 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,245 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,245 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,246 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,247 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,247 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,248 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,248 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,249 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,250 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,250 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,251 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,251 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,252 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,253 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,253 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,254 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,254 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,255 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,256 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,256 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,257 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,258 - mmcv - INFO - \n",
      "reid.head.fcs.0.fc.weight - torch.Size([1024, 2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,258 - mmcv - INFO - \n",
      "reid.head.fcs.0.fc.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,259 - mmcv - INFO - \n",
      "reid.head.fcs.0.bn.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,259 - mmcv - INFO - \n",
      "reid.head.fcs.0.bn.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,260 - mmcv - INFO - \n",
      "reid.head.fc_out.weight - torch.Size([128, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,261 - mmcv - INFO - \n",
      "reid.head.fc_out.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,261 - mmcv - INFO - \n",
      "reid.head.bn.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,262 - mmcv - INFO - \n",
      "reid.head.bn.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,263 - mmcv - INFO - \n",
      "reid.head.classifier.weight - torch.Size([380, 128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-21 16:28:15,263 - mmcv - INFO - \n",
      "reid.head.classifier.bias - torch.Size([380]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 823/823, 5.8 task/s, elapsed: 143s, ETA:     0sEvaluate CLEAR MOT results.\n",
      "\n",
      "Eval Config:\n",
      "USE_PARALLEL         : False                         \n",
      "NUM_PARALLEL_CORES   : 8                             \n",
      "BREAK_ON_ERROR       : True                          \n",
      "RETURN_ON_ERROR      : False                         \n",
      "LOG_ON_ERROR         : /environment/miniconda3/lib/python3.7/site-packages/error_log.txt\n",
      "PRINT_RESULTS        : True                          \n",
      "PRINT_ONLY_COMBINED  : False                         \n",
      "PRINT_CONFIG         : True                          \n",
      "TIME_PROGRESS        : True                          \n",
      "DISPLAY_LESS_PROGRESS : True                          \n",
      "OUTPUT_SUMMARY       : True                          \n",
      "OUTPUT_EMPTY_CLASSES : True                          \n",
      "OUTPUT_DETAILED      : True                          \n",
      "PLOT_CURVES          : True                          \n",
      "\n",
      "MotChallenge2DBox Config:\n",
      "GT_FOLDER            : data/MOT17_tiny/train         \n",
      "TRACKERS_FOLDER      : /tmp/tmpfolga75r              \n",
      "OUTPUT_FOLDER        : None                          \n",
      "TRACKERS_TO_EVAL     : ['track']                     \n",
      "CLASSES_TO_EVAL      : ['pedestrian']                \n",
      "BENCHMARK            : MOT17                         \n",
      "SPLIT_TO_EVAL        : train                         \n",
      "INPUT_AS_ZIP         : False                         \n",
      "PRINT_CONFIG         : True                          \n",
      "DO_PREPROC           : True                          \n",
      "TRACKER_SUB_FOLDER   :                               \n",
      "OUTPUT_SUB_FOLDER    :                               \n",
      "TRACKER_DISPLAY_NAMES : None                          \n",
      "SEQMAP_FOLDER        : None                          \n",
      "SEQMAP_FILE          : /tmp/tmpfolga75r/videoseq.txt \n",
      "SEQ_INFO             : None                          \n",
      "GT_LOC_FORMAT        : {gt_folder}/{seq}/gt/gt_half-val.txt\n",
      "SKIP_SPLIT_FOL       : True                          \n",
      "\n",
      "Evaluating 1 tracker(s) on 2 sequence(s) for 1 class(es) on MotChallenge2DBox dataset using the following metrics: HOTA, Count\n",
      "\n",
      "\n",
      "Evaluating track\n",
      "\n",
      "1 eval_sequence(MOT17-02-FRCNN, track)                                   0.4788 sec\n",
      "2 eval_sequence(MOT17-04-FRCNN, track)                                   1.2682 sec\n",
      "\n",
      "All sequences for track finished in 1.75 seconds\n",
      "\n",
      "HOTA: track-pedestrian             HOTA      DetA      AssA      DetRe     DetPr     AssRe     AssPr     LocA      RHOTA     HOTA(0)   LocA(0)   HOTALocA(0)\n",
      "MOT17-02-FRCNN                     15.63     24.542    10.399    34.786    38.517    11.559    46.377    72.035    18.754    24.891    54.4      13.541    \n",
      "MOT17-04-FRCNN                     33.527    43.414    27.041    53.582    60.066    31.077    51.368    78.257    37.56     45.347    68.397    31.016    \n",
      "COMBINED                           29.022    37.292    23.943    48.129    53.76     27.543    50.787    76.903    33.289    40.027    64.871    25.966    \n",
      "\n",
      "Count: track-pedestrian            Dets      GT_Dets   IDs       GT_IDs    \n",
      "MOT17-02-FRCNN                     8923      9880      839       53        \n",
      "MOT17-04-FRCNN                     21568     24178     444       69        \n",
      "COMBINED                           30491     34058     1283      122       \n",
      "                IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML    FP    FN  IDs    FM   MOTA  MOTP IDt IDa IDm      HOTA\n",
      "MOT17-02-FRCNN 17.6% 18.6% 16.8% 42.3% 46.8%  53  6 27 20  4748  5705 1017   428 -16.1% 0.302 313 309   5  0.156295\n",
      "MOT17-04-FRCNN 37.3% 39.5% 35.3% 65.4% 73.3%  69 21 43  5  5761  8371 1585   662  35.0% 0.239 502 180  12  0.335268\n",
      "OVERALL        31.6% 33.4% 29.9% 58.7% 65.5% 122 27 70 25 10509 14076 2602  1090  20.2% 0.252 815 489  17  0.290223\n",
      "{'IDF1': 0.316, 'IDP': 0.334, 'IDR': 0.299, 'Rcll': 0.587, 'Prcn': 0.655, 'GT': 122, 'MT': 27, 'PT': 70, 'ML': 25, 'FP': 10509, 'FN': 14076, 'IDs': 2602, 'FM': 1090, 'MOTA': 0.202, 'MOTP': 0.252, 'IDt': 815, 'IDa': 489, 'IDm': 17, 'HOTA': 0.29}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mmtrack.datasets import build_dataloader\n",
    "from mmtrack.models import build_model\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmtrack.apis import single_gpu_test\n",
    "from mmtrack.datasets import build_dataset\n",
    "\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=1,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=False,\n",
    "    shuffle=False)\n",
    "\n",
    "# build the model and load checkpoint\n",
    "model = build_model(cfg.model)\n",
    "model.init_weights()\n",
    "\n",
    "model = MMDataParallel(model, device_ids=cfg.gpu_ids)\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_kwargs = cfg.get('evaluation', {}).copy()\n",
    "# hard-code way to remove EvalHook args\n",
    "eval_hook_args = [\n",
    "    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',\n",
    "    'rule', 'by_epoch'\n",
    "]\n",
    "for key in eval_hook_args:\n",
    "    eval_kwargs.pop(key, None)\n",
    "eval_kwargs.update(dict(metric=['track']))\n",
    "metric = dataset.evaluate(outputs, **eval_kwargs)\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45d329-7ce0-485e-8dad-b57cb7691534",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tools/train.py \\\n",
    "        config/XXX.py \\\n",
    "        --work-dir \\\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
