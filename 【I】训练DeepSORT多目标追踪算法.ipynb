{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee09ba2-d6c3-44d1-adfa-2299d3702d7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 训练 DeepSORT 多目标追踪算法\n",
    "\n",
    "如果报错`CUDA out of memory.`则重启前面几个代码的`kernel`即可。\n",
    "\n",
    "作者：同济子豪兄、张经伟 2022-4-25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84dccb-940a-4115-980b-2f6d50fcaed3",
   "metadata": {},
   "source": [
    "## 进入 MMTracking 主目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad8307f3-4150-48bb-96d2-b2836d7499ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.circleci',\n",
       " '.dev_scripts',\n",
       " '.github',\n",
       " '.gitignore',\n",
       " '.pre-commit-config.yaml',\n",
       " '.readthedocs.yml',\n",
       " 'CITATION.cff',\n",
       " 'LICENSE',\n",
       " 'MANIFEST.in',\n",
       " 'README.md',\n",
       " 'README_zh-CN.md',\n",
       " 'configs',\n",
       " 'demo',\n",
       " 'docker',\n",
       " 'docs',\n",
       " 'mmtrack',\n",
       " 'model-index.yml',\n",
       " 'requirements.txt',\n",
       " 'requirements',\n",
       " 'resources',\n",
       " 'setup.cfg',\n",
       " 'setup.py',\n",
       " 'tests',\n",
       " 'tools',\n",
       " 'mmtrack.egg-info',\n",
       " 'checkpoints',\n",
       " 'outputs',\n",
       " 'data',\n",
       " '20220425141741',\n",
       " '20220425141741-plot',\n",
       " 'test.jpg']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('mmtracking')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddc6cd4-af1d-4243-bcc1-ef9dd2ca8be7",
   "metadata": {},
   "source": [
    "## 多目标追踪数据集准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8cf8e-54fb-431d-8f1c-bb496bb0c4e3",
   "metadata": {},
   "source": [
    "### 下载 MOT17 多目标追踪数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad21a574-2602-4fa3-a46b-b4c3bed15c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-25 09:11:47--  https://download.openmmlab.com/mmtracking/data/MOT17_tiny.zip\n",
      "Connecting to 172.16.0.13:5848... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 344566302 (329M) [application/zip]\n",
      "Saving to: ‘./data/MOT17_tiny.zip’\n",
      "\n",
      "MOT17_tiny.zip      100%[===================>] 328.60M  86.8MB/s    in 3.8s    \n",
      "\n",
      "2022-04-25 09:11:51 (86.7 MB/s) - ‘./data/MOT17_tiny.zip’ saved [344566302/344566302]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 下载 MOT17 数据集压缩包\n",
    "!wget https://download.openmmlab.com/mmtracking/data/MOT17_tiny.zip -P ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4cd48d-acd0-4d14-a6b0-562763105ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./data/MOT17_tiny.zip\n",
      "   creating: ./data/MOT17_tiny/\n",
      "   creating: ./data/MOT17_tiny/test/\n",
      "   creating: ./data/MOT17_tiny/train/\n",
      "   creating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/\n",
      "   creating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/det/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/det/det.txt  \n",
      "   creating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/gt/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/gt/gt.txt  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/gt/gt_half-train.txt  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/gt/gt_half-val.txt  \n",
      "   creating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000001.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000002.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000003.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000004.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000005.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000006.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000007.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000008.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000009.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000010.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000011.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000012.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000013.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000014.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000015.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000016.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000017.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000018.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000019.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000020.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000021.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000022.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000023.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000024.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000025.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000026.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000027.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000028.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000029.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000030.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000031.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000032.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000033.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000034.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000035.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000036.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000037.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000038.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000039.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000040.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000041.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000042.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000043.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000044.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000045.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000046.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000047.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000048.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000049.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000050.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000051.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000052.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000053.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000054.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000055.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000056.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000057.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000058.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000059.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000060.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000061.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000062.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000063.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000064.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000065.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000066.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000067.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000068.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000069.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000070.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000071.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000072.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000073.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000074.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000075.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000076.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000077.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000078.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000079.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000080.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000081.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000082.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000083.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000084.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000085.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000086.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000087.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000088.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000089.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000090.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000091.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000092.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000093.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000094.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000095.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000096.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000097.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000098.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000099.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000100.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000101.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000102.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000103.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000104.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000105.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000106.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000107.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000108.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000109.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000110.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000111.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000112.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000113.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000114.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000115.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000116.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000117.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000118.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000119.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000120.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000121.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000122.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000123.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000124.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000125.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000126.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000127.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000128.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000129.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000130.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000131.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000132.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000133.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000134.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000135.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000136.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000137.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000138.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000139.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000140.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000141.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000142.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000143.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000144.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000145.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000146.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000147.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000148.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000149.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000150.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000151.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000152.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000153.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000154.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000155.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000156.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000157.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000158.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000159.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000160.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000161.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000162.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000163.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000164.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000165.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000166.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000167.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000168.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000169.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000170.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000171.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000172.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000173.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000174.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000175.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000176.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000177.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000178.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000179.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000180.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000181.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000182.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000183.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000184.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000185.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000186.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000187.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000188.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000189.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000190.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000191.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000192.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000193.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000194.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000195.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000196.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000197.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000198.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000199.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000200.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000201.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000202.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000203.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000204.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000205.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000206.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000207.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000208.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000209.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000210.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000211.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000212.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000213.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000214.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000215.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000216.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000217.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000218.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000219.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000220.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000221.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000222.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000223.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000224.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000225.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000226.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000227.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000228.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000229.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000230.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000231.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000232.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000233.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000234.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000235.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000236.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000237.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000238.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000239.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000240.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000241.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000242.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000243.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000244.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000245.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000246.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000247.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000248.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000249.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000250.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000251.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000252.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000253.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000254.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000255.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000256.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000257.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000258.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000259.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000260.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000261.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000262.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000263.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000264.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000265.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000266.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000267.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000268.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000269.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000270.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000271.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000272.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000273.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000274.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000275.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000276.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000277.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000278.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000279.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000280.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000281.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000282.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000283.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000284.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000285.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000286.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000287.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000288.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000289.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000290.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000291.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000292.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000293.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000294.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000295.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000296.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000297.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000298.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000299.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000300.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000301.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000302.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000303.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000304.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000305.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000306.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000307.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000308.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000309.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000310.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000311.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000312.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000313.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000314.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000315.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000316.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000317.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000318.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000319.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000320.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000321.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000322.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000323.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000324.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000325.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000326.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000327.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000328.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000329.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000330.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000331.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000332.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000333.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000334.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000335.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000336.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000337.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000338.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000339.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000340.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000341.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000342.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000343.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000344.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000345.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000346.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000347.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000348.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000349.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000350.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000351.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000352.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000353.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000354.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000355.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000356.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000357.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000358.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000359.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000360.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000361.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000362.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000363.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000364.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000365.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000366.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000367.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000368.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000369.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000370.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000371.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000372.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000373.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000374.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000375.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000376.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000377.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000378.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000379.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000380.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000381.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000382.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000383.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000384.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000385.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000386.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000387.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000388.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000389.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000390.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000391.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000392.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000393.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000394.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000395.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000396.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000397.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000398.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000399.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000400.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000401.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000402.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000403.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000404.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000405.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000406.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000407.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000408.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000409.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000410.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000411.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000412.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000413.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000414.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000415.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000416.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000417.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000418.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000419.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000420.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000421.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000422.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000423.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000424.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000425.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000426.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000427.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000428.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000429.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000430.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000431.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000432.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000433.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000434.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000435.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000436.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000437.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000438.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000439.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000440.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000441.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000442.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000443.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000444.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000445.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000446.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000447.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000448.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000449.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000450.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000451.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000452.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000453.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000454.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000455.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000456.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000457.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000458.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000459.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000460.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000461.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000462.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000463.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000464.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000465.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000466.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000467.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000468.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000469.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000470.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000471.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000472.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000473.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000474.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000475.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000476.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000477.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000478.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000479.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000480.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000481.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000482.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000483.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000484.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000485.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000486.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000487.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000488.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000489.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000490.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000491.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000492.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000493.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000494.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000495.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000496.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000497.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000498.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000499.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000500.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000501.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000502.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000503.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000504.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000505.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000506.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000507.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000508.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000509.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000510.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000511.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000512.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000513.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000514.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000515.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000516.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000517.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000518.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000519.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000520.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000521.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000522.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000523.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000524.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000525.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000526.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000527.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000528.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000529.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000530.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000531.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000532.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000533.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000534.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000535.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000536.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000537.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000538.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000539.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000540.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000541.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000542.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000543.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000544.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000545.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000546.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000547.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000548.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000549.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000550.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000551.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000552.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000553.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000554.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000555.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000556.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000557.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000558.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000559.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000560.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000561.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000562.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000563.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000564.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000565.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000566.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000567.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000568.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000569.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000570.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000571.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000572.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000573.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000574.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000575.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000576.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000577.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000578.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000579.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000580.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000581.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000582.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000583.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000584.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000585.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000586.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000587.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000588.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000589.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000590.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000591.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000592.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000593.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000594.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000595.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000596.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000597.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000598.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000599.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/img1/000600.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-02-FRCNN/seqinfo.ini  \n",
      "   creating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/\n",
      "   creating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/det/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/det/det.txt  \n",
      "   creating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/gt/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/gt/gt.txt  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/gt/gt_half-train.txt  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/gt/gt_half-val.txt  \n",
      "   creating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/\n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000001.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000002.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000003.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000004.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000005.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000006.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000007.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000008.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000009.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000010.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000011.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000012.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000013.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000014.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000015.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000016.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000017.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000018.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000019.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000020.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000021.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000022.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000023.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000024.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000025.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000026.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000027.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000028.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000029.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000030.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000031.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000032.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000033.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000034.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000035.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000036.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000037.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000038.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000039.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000040.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000041.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000042.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000043.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000044.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000045.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000046.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000047.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000048.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000049.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000050.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000051.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000052.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000053.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000054.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000055.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000056.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000057.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000058.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000059.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000060.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000061.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000062.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000063.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000064.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000065.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000066.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000067.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000068.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000069.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000070.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000071.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000072.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000073.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000074.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000075.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000076.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000077.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000078.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000079.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000080.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000081.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000082.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000083.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000084.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000085.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000086.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000087.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000088.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000089.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000090.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000091.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000092.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000093.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000094.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000095.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000096.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000097.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000098.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000099.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000100.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000101.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000102.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000103.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000104.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000105.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000106.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000107.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000108.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000109.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000110.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000111.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000112.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000113.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000114.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000115.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000116.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000117.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000118.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000119.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000120.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000121.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000122.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000123.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000124.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000125.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000126.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000127.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000128.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000129.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000130.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000131.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000132.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000133.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000134.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000135.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000136.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000137.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000138.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000139.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000140.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000141.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000142.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000143.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000144.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000145.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000146.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000147.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000148.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000149.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000150.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000151.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000152.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000153.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000154.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000155.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000156.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000157.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000158.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000159.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000160.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000161.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000162.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000163.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000164.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000165.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000166.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000167.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000168.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000169.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000170.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000171.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000172.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000173.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000174.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000175.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000176.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000177.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000178.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000179.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000180.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000181.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000182.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000183.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000184.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000185.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000186.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000187.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000188.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000189.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000190.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000191.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000192.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000193.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000194.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000195.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000196.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000197.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000198.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000199.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000200.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000201.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000202.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000203.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000204.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000205.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000206.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000207.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000208.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000209.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000210.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000211.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000212.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000213.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000214.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000215.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000216.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000217.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000218.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000219.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000220.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000221.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000222.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000223.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000224.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000225.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000226.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000227.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000228.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000229.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000230.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000231.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000232.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000233.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000234.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000235.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000236.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000237.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000238.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000239.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000240.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000241.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000242.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000243.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000244.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000245.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000246.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000247.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000248.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000249.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000250.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000251.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000252.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000253.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000254.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000255.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000256.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000257.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000258.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000259.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000260.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000261.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000262.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000263.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000264.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000265.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000266.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000267.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000268.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000269.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000270.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000271.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000272.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000273.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000274.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000275.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000276.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000277.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000278.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000279.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000280.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000281.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000282.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000283.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000284.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000285.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000286.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000287.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000288.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000289.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000290.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000291.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000292.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000293.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000294.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000295.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000296.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000297.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000298.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000299.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000300.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000301.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000302.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000303.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000304.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000305.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000306.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000307.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000308.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000309.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000310.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000311.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000312.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000313.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000314.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000315.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000316.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000317.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000318.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000319.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000320.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000321.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000322.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000323.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000324.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000325.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000326.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000327.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000328.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000329.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000330.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000331.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000332.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000333.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000334.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000335.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000336.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000337.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000338.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000339.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000340.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000341.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000342.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000343.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000344.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000345.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000346.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000347.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000348.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000349.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000350.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000351.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000352.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000353.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000354.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000355.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000356.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000357.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000358.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000359.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000360.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000361.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000362.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000363.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000364.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000365.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000366.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000367.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000368.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000369.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000370.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000371.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000372.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000373.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000374.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000375.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000376.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000377.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000378.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000379.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000380.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000381.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000382.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000383.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000384.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000385.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000386.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000387.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000388.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000389.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000390.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000391.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000392.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000393.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000394.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000395.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000396.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000397.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000398.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000399.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000400.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000401.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000402.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000403.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000404.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000405.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000406.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000407.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000408.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000409.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000410.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000411.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000412.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000413.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000414.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000415.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000416.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000417.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000418.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000419.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000420.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000421.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000422.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000423.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000424.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000425.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000426.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000427.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000428.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000429.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000430.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000431.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000432.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000433.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000434.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000435.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000436.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000437.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000438.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000439.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000440.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000441.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000442.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000443.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000444.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000445.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000446.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000447.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000448.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000449.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000450.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000451.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000452.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000453.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000454.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000455.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000456.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000457.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000458.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000459.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000460.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000461.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000462.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000463.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000464.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000465.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000466.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000467.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000468.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000469.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000470.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000471.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000472.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000473.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000474.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000475.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000476.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000477.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000478.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000479.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000480.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000481.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000482.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000483.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000484.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000485.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000486.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000487.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000488.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000489.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000490.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000491.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000492.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000493.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000494.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000495.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000496.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000497.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000498.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000499.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000500.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000501.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000502.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000503.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000504.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000505.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000506.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000507.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000508.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000509.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000510.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000511.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000512.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000513.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000514.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000515.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000516.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000517.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000518.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000519.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000520.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000521.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000522.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000523.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000524.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000525.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000526.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000527.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000528.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000529.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000530.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000531.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000532.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000533.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000534.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000535.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000536.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000537.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000538.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000539.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000540.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000541.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000542.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000543.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000544.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000545.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000546.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000547.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000548.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000549.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000550.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000551.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000552.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000553.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000554.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000555.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000556.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000557.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000558.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000559.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000560.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000561.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000562.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000563.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000564.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000565.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000566.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000567.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000568.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000569.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000570.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000571.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000572.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000573.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000574.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000575.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000576.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000577.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000578.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000579.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000580.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000581.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000582.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000583.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000584.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000585.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000586.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000587.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000588.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000589.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000590.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000591.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000592.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000593.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000594.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000595.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000596.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000597.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000598.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000599.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000600.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000601.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000602.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000603.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000604.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000605.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000606.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000607.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000608.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000609.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000610.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000611.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000612.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000613.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000614.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000615.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000616.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000617.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000618.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000619.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000620.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000621.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000622.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000623.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000624.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000625.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000626.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000627.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000628.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000629.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000630.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000631.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000632.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000633.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000634.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000635.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000636.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000637.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000638.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000639.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000640.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000641.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000642.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000643.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000644.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000645.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000646.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000647.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000648.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000649.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000650.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000651.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000652.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000653.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000654.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000655.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000656.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000657.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000658.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000659.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000660.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000661.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000662.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000663.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000664.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000665.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000666.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000667.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000668.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000669.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000670.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000671.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000672.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000673.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000674.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000675.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000676.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000677.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000678.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000679.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000680.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000681.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000682.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000683.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000684.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000685.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000686.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000687.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000688.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000689.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000690.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000691.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000692.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000693.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000694.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000695.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000696.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000697.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000698.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000699.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000700.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000701.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000702.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000703.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000704.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000705.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000706.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000707.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000708.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000709.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000710.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000711.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000712.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000713.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000714.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000715.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000716.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000717.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000718.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000719.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000720.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000721.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000722.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000723.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000724.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000725.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000726.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000727.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000728.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000729.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000730.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000731.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000732.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000733.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000734.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000735.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000736.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000737.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000738.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000739.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000740.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000741.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000742.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000743.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000744.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000745.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000746.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000747.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000748.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000749.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000750.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000751.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000752.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000753.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000754.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000755.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000756.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000757.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000758.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000759.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000760.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000761.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000762.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000763.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000764.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000765.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000766.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000767.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000768.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000769.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000770.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000771.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000772.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000773.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000774.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000775.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000776.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000777.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000778.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000779.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000780.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000781.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000782.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000783.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000784.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000785.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000786.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000787.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000788.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000789.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000790.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000791.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000792.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000793.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000794.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000795.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000796.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000797.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000798.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000799.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000800.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000801.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000802.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000803.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000804.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000805.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000806.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000807.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000808.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000809.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000810.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000811.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000812.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000813.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000814.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000815.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000816.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000817.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000818.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000819.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000820.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000821.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000822.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000823.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000824.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000825.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000826.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000827.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000828.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000829.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000830.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000831.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000832.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000833.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000834.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000835.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000836.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000837.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000838.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000839.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000840.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000841.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000842.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000843.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000844.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000845.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000846.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000847.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000848.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000849.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000850.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000851.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000852.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000853.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000854.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000855.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000856.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000857.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000858.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000859.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000860.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000861.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000862.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000863.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000864.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000865.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000866.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000867.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000868.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000869.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000870.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000871.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000872.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000873.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000874.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000875.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000876.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000877.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000878.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000879.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000880.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000881.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000882.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000883.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000884.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000885.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000886.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000887.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000888.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000889.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000890.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000891.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000892.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000893.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000894.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000895.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000896.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000897.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000898.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000899.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000900.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000901.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000902.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000903.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000904.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000905.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000906.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000907.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000908.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000909.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000910.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000911.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000912.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000913.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000914.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000915.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000916.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000917.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000918.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000919.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000920.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000921.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000922.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000923.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000924.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000925.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000926.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000927.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000928.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000929.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000930.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000931.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000932.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000933.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000934.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000935.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000936.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000937.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000938.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000939.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000940.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000941.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000942.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000943.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000944.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000945.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000946.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000947.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000948.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000949.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000950.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000951.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000952.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000953.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000954.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000955.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000956.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000957.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000958.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000959.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000960.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000961.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000962.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000963.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000964.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000965.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000966.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000967.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000968.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000969.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000970.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000971.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000972.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000973.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000974.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000975.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000976.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000977.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000978.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000979.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000980.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000981.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000982.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000983.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000984.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000985.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000986.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000987.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000988.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000989.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000990.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000991.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000992.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000993.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000994.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000995.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000996.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000997.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000998.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/000999.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001000.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001001.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001002.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001003.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001004.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001005.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001006.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001007.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001008.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001009.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001010.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001011.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001012.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001013.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001014.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001015.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001016.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001017.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001018.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001019.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001020.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001021.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001022.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001023.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001024.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001025.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001026.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001027.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001028.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001029.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001030.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001031.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001032.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001033.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001034.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001035.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001036.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001037.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001038.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001039.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001040.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001041.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001042.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001043.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001044.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001045.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001046.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001047.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001048.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001049.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/img1/001050.jpg  \n",
      "  inflating: ./data/MOT17_tiny/train/MOT17-04-FRCNN/seqinfo.ini  \n"
     ]
    }
   ],
   "source": [
    "# 解压\n",
    "!rm -rf data/MOT17_tiny\n",
    "!unzip ./data/MOT17_tiny.zip -d ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd41fd-195a-4e19-8404-d508ddaf286f",
   "metadata": {},
   "source": [
    "### 将 txt 格式的标注文件，转换为 coco 格式的 json 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372314b8-a9b7-406e-9423-9faf4aea005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train set to COCO format\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.29it/s]\n",
      "train has 145 instances.\n",
      "Done! Saved as ./data/MOT17_tiny/annotations/train_cocoformat.json and ./data/MOT17_tiny/annotations/train_detections.pkl\n",
      "Converting test set to COCO format\n",
      "0it [00:00, ?it/s]\n",
      "test has 0 instances.\n",
      "Done! Saved as ./data/MOT17_tiny/annotations/test_cocoformat.json and ./data/MOT17_tiny/annotations/test_detections.pkl\n",
      "Converting half-train set to COCO format\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.15s/it]\n",
      "half-train has 104 instances.\n",
      "Done! Saved as ./data/MOT17_tiny/annotations/half-train_cocoformat.json and ./data/MOT17_tiny/annotations/half-train_detections.pkl\n",
      "Converting half-val set to COCO format\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:02<00:00,  1.12s/it]\n",
      "half-val has 122 instances.\n",
      "Done! Saved as ./data/MOT17_tiny/annotations/half-val_cocoformat.json and ./data/MOT17_tiny/annotations/half-val_detections.pkl\n"
     ]
    }
   ],
   "source": [
    "!python ./tools/convert_datasets/mot/mot2coco.py -i ./data/MOT17_tiny/ -o ./data/MOT17_tiny/annotations --split-train --convert-det"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb571e71-0ae6-4e14-829c-a0e2c1102c9a",
   "metadata": {},
   "source": [
    "### 从数据集中把行人的图块裁剪出来，用于训练 ReID 重识别模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43f952e-d06f-46da-a590-53a6d6b33d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除已有的 reid 目录（如有）\n",
    "!rm -rf ./data/MOT17_tiny/reid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6fae6b-a8cf-43f6-81fb-a21306a99dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 2/2 [20:51<00:00, 625.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# 大概需要 20 分钟\n",
    "!python ./tools/convert_datasets/mot/mot2reid.py -i ./data/MOT17_tiny/ -o ./data/MOT17_tiny/reid --val-split 0.9 --vis-threshold 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21efa18-0968-4e32-a294-3946e225060b",
   "metadata": {},
   "source": [
    "## 分别训练 DeepSORT 模型中的不同模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1364b76-ac1e-4da2-bce6-0f06527fc3f0",
   "metadata": {},
   "source": [
    "### 配置 DeepSORT 中的目标检测算法模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e6b92d3-80bc-4253-bf59-27d4811586a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "cfg = mmcv.Config.fromfile('./configs/det/faster-rcnn_r50_fpn_4e_mot17-half.py')\n",
    "cfg.data_root = 'data/MOT17_tiny/'\n",
    "cfg.data.test.ann_file = cfg.data.test.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.ann_file = cfg.data.train.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.val.ann_file = cfg.data.val.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "cfg.data.test.img_prefix = cfg.data.test.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.img_prefix = cfg.data.train.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.val.img_prefix = cfg.data.val.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "cfg.work_dir = './tutorial_exps/detector'\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e78c3273-be10-45c7-80a0-c769899edfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = dict(\n",
      "    detector=dict(\n",
      "        type='FasterRCNN',\n",
      "        backbone=dict(\n",
      "            type='ResNet',\n",
      "            depth=50,\n",
      "            num_stages=4,\n",
      "            out_indices=(0, 1, 2, 3),\n",
      "            frozen_stages=1,\n",
      "            norm_cfg=dict(type='BN', requires_grad=True),\n",
      "            norm_eval=True,\n",
      "            style='pytorch',\n",
      "            init_cfg=dict(\n",
      "                type='Pretrained', checkpoint='torchvision://resnet50')),\n",
      "        neck=dict(\n",
      "            type='FPN',\n",
      "            in_channels=[256, 512, 1024, 2048],\n",
      "            out_channels=256,\n",
      "            num_outs=5),\n",
      "        rpn_head=dict(\n",
      "            type='RPNHead',\n",
      "            in_channels=256,\n",
      "            feat_channels=256,\n",
      "            anchor_generator=dict(\n",
      "                type='AnchorGenerator',\n",
      "                scales=[8],\n",
      "                ratios=[0.5, 1.0, 2.0],\n",
      "                strides=[4, 8, 16, 32, 64]),\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[1.0, 1.0, 1.0, 1.0],\n",
      "                clip_border=False),\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "            loss_bbox=dict(\n",
      "                type='SmoothL1Loss', beta=0.1111111111111111,\n",
      "                loss_weight=1.0)),\n",
      "        roi_head=dict(\n",
      "            type='StandardRoIHead',\n",
      "            bbox_roi_extractor=dict(\n",
      "                type='SingleRoIExtractor',\n",
      "                roi_layer=dict(\n",
      "                    type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "                out_channels=256,\n",
      "                featmap_strides=[4, 8, 16, 32]),\n",
      "            bbox_head=dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=1,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.1, 0.1, 0.2, 0.2],\n",
      "                    clip_border=False),\n",
      "                reg_class_agnostic=False,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', loss_weight=1.0))),\n",
      "        train_cfg=dict(\n",
      "            rpn=dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.7,\n",
      "                    neg_iou_thr=0.3,\n",
      "                    min_pos_iou=0.3,\n",
      "                    match_low_quality=True,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=256,\n",
      "                    pos_fraction=0.5,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=False),\n",
      "                allowed_border=-1,\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            rpn_proposal=dict(\n",
      "                nms_pre=2000,\n",
      "                max_per_img=1000,\n",
      "                nms=dict(type='nms', iou_threshold=0.7),\n",
      "                min_bbox_size=0),\n",
      "            rcnn=dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    min_pos_iou=0.5,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False)),\n",
      "        test_cfg=dict(\n",
      "            rpn=dict(\n",
      "                nms_pre=1000,\n",
      "                max_per_img=1000,\n",
      "                nms=dict(type='nms', iou_threshold=0.7),\n",
      "                min_bbox_size=0),\n",
      "            rcnn=dict(\n",
      "                score_thr=0.05,\n",
      "                nms=dict(type='nms', iou_threshold=0.5),\n",
      "                max_per_img=100)),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint=\n",
      "            'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
      "        )))\n",
      "dataset_type = 'CocoDataset'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile', to_float32=True),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        type='Resize',\n",
      "        img_scale=(1088, 1088),\n",
      "        ratio_range=(0.8, 1.2),\n",
      "        keep_ratio=True,\n",
      "        bbox_clip_border=False),\n",
      "    dict(type='PhotoMetricDistortion'),\n",
      "    dict(type='RandomCrop', crop_size=(1088, 1088), bbox_clip_border=False),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1088, 1088),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data_root = 'data/MOT17_tiny/'\n",
      "data = dict(\n",
      "    samples_per_gpu=2,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-train_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        classes=('pedestrian', ),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile', to_float32=True),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                type='Resize',\n",
      "                img_scale=(1088, 1088),\n",
      "                ratio_range=(0.8, 1.2),\n",
      "                keep_ratio=True,\n",
      "                bbox_clip_border=False),\n",
      "            dict(type='PhotoMetricDistortion'),\n",
      "            dict(\n",
      "                type='RandomCrop',\n",
      "                crop_size=(1088, 1088),\n",
      "                bbox_clip_border=False),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        classes=('pedestrian', ),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1088, 1088),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        classes=('pedestrian', ),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1088, 1088),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]))\n",
      "evaluation = dict(metric=['bbox'])\n",
      "optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "USE_MMDET = True\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=100,\n",
      "    warmup_ratio=0.01,\n",
      "    step=[3])\n",
      "total_epochs = 4\n",
      "work_dir = './tutorial_exps/detector'\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1367154-8d39-4d49-8a59-9450483cbdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.26s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmtrack.datasets import build_dataset\n",
    "from mmdet.apis import train_detector as train_model\n",
    "from mmdet.models import build_detector as build_model\n",
    "\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "model = build_model(cfg.model.detector)\n",
    "\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "model.CLASSES = datasets[0].CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6965faa-626b-4a2b-998b-c77e56edf892",
   "metadata": {},
   "source": [
    "### 训练 DeepSORT 中的目标检测算法模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74e1f9b0-9219-4884-bb61-67a3db7282b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5397334-f4b6-4a11-bd5c-6a3d3a871e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.7/site-packages/mmdet/apis/train.py:135: UserWarning: config is now expected to have a `runner` section, please set `runner` in your config.\n",
      "  'please set `runner` in your config.', UserWarning)\n",
      "2022-04-25 09:37:10,410 - mmdet - INFO - Start running, host: featurize@featurize, work_dir: /home/featurize/work/MMTracking教程/0421/mmtracking/tutorial_exps/detector\n",
      "2022-04-25 09:37:10,411 - mmdet - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-04-25 09:37:10,412 - mmdet - INFO - workflow: [('train', 1)], max: 4 epochs\n",
      "2022-04-25 09:37:10,413 - mmdet - INFO - Checkpoints will be saved to /home/featurize/work/MMTracking教程/0421/mmtracking/tutorial_exps/detector by HardDiskBackend.\n",
      "2022-04-25 09:37:27,693 - mmdet - INFO - Epoch [1][50/414]\tlr: 9.902e-03, eta: 0:09:12, time: 0.344, data_time: 0.051, memory: 2944, loss_rpn_cls: 0.5058, loss_rpn_bbox: 0.2429, loss_cls: 0.3806, acc: 85.3828, loss_bbox: 0.1002, loss: 1.2295\n",
      "2022-04-25 09:37:42,816 - mmdet - INFO - Epoch [1][100/414]\tlr: 1.980e-02, eta: 0:08:22, time: 0.302, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.3312, loss_rpn_bbox: 0.2162, loss_cls: 0.3978, acc: 83.4844, loss_bbox: 0.1939, loss: 1.1390\n",
      "2022-04-25 09:37:57,810 - mmdet - INFO - Epoch [1][150/414]\tlr: 2.000e-02, eta: 0:07:55, time: 0.300, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.3398, loss_rpn_bbox: 0.2016, loss_cls: 0.3583, acc: 85.3418, loss_bbox: 0.1558, loss: 1.0555\n",
      "2022-04-25 09:38:13,047 - mmdet - INFO - Epoch [1][200/414]\tlr: 2.000e-02, eta: 0:07:35, time: 0.305, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.3154, loss_rpn_bbox: 0.1976, loss_cls: 0.3588, acc: 85.6621, loss_bbox: 0.1641, loss: 1.0359\n",
      "2022-04-25 09:38:28,172 - mmdet - INFO - Epoch [1][250/414]\tlr: 2.000e-02, eta: 0:07:16, time: 0.302, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2679, loss_rpn_bbox: 0.1675, loss_cls: 0.3555, acc: 84.7539, loss_bbox: 0.2079, loss: 0.9988\n",
      "2022-04-25 09:38:43,100 - mmdet - INFO - Epoch [1][300/414]\tlr: 2.000e-02, eta: 0:06:58, time: 0.298, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2483, loss_rpn_bbox: 0.1663, loss_cls: 0.3440, acc: 85.0273, loss_bbox: 0.2188, loss: 0.9774\n",
      "2022-04-25 09:38:58,088 - mmdet - INFO - Epoch [1][350/414]\tlr: 2.000e-02, eta: 0:06:41, time: 0.300, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2514, loss_rpn_bbox: 0.1609, loss_cls: 0.3384, acc: 85.2695, loss_bbox: 0.2114, loss: 0.9622\n",
      "2022-04-25 09:39:13,121 - mmdet - INFO - Epoch [1][400/414]\tlr: 2.000e-02, eta: 0:06:25, time: 0.301, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2746, loss_rpn_bbox: 0.1812, loss_cls: 0.3433, acc: 85.5859, loss_bbox: 0.1996, loss: 0.9987\n",
      "2022-04-25 09:39:17,231 - mmdet - INFO - Saving checkpoint at 1 epochs\n",
      "2022-04-25 09:39:41,909 - mmdet - INFO - Epoch [2][50/414]\tlr: 2.000e-02, eta: 0:05:59, time: 0.344, data_time: 0.051, memory: 2944, loss_rpn_cls: 0.2214, loss_rpn_bbox: 0.1495, loss_cls: 0.3340, acc: 85.7207, loss_bbox: 0.2116, loss: 0.9165\n",
      "2022-04-25 09:39:57,137 - mmdet - INFO - Epoch [2][100/414]\tlr: 2.000e-02, eta: 0:05:44, time: 0.304, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.2285, loss_rpn_bbox: 0.1653, loss_cls: 0.3251, acc: 85.9512, loss_bbox: 0.2033, loss: 0.9222\n",
      "2022-04-25 09:40:12,449 - mmdet - INFO - Epoch [2][150/414]\tlr: 2.000e-02, eta: 0:05:29, time: 0.306, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.1995, loss_rpn_bbox: 0.1544, loss_cls: 0.3357, acc: 85.9727, loss_bbox: 0.2096, loss: 0.8991\n",
      "2022-04-25 09:40:28,062 - mmdet - INFO - Epoch [2][200/414]\tlr: 2.000e-02, eta: 0:05:15, time: 0.312, data_time: 0.016, memory: 2944, loss_rpn_cls: 0.2262, loss_rpn_bbox: 0.1406, loss_cls: 0.3172, acc: 86.4727, loss_bbox: 0.1979, loss: 0.8820\n",
      "2022-04-25 09:40:43,079 - mmdet - INFO - Epoch [2][250/414]\tlr: 2.000e-02, eta: 0:05:00, time: 0.300, data_time: 0.009, memory: 2944, loss_rpn_cls: 0.2017, loss_rpn_bbox: 0.1486, loss_cls: 0.3105, acc: 87.0820, loss_bbox: 0.2069, loss: 0.8676\n",
      "2022-04-25 09:40:58,206 - mmdet - INFO - Epoch [2][300/414]\tlr: 2.000e-02, eta: 0:04:45, time: 0.302, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1868, loss_rpn_bbox: 0.1482, loss_cls: 0.3139, acc: 86.6133, loss_bbox: 0.2152, loss: 0.8641\n",
      "2022-04-25 09:41:13,378 - mmdet - INFO - Epoch [2][350/414]\tlr: 2.000e-02, eta: 0:04:30, time: 0.304, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1999, loss_rpn_bbox: 0.1427, loss_cls: 0.3121, acc: 86.6348, loss_bbox: 0.2106, loss: 0.8653\n",
      "2022-04-25 09:41:28,579 - mmdet - INFO - Epoch [2][400/414]\tlr: 2.000e-02, eta: 0:04:14, time: 0.304, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1773, loss_rpn_bbox: 0.1465, loss_cls: 0.3101, acc: 86.9844, loss_bbox: 0.2093, loss: 0.8433\n",
      "2022-04-25 09:41:32,684 - mmdet - INFO - Saving checkpoint at 2 epochs\n",
      "2022-04-25 09:41:50,942 - mmdet - INFO - Epoch [3][50/414]\tlr: 2.000e-02, eta: 0:03:53, time: 0.340, data_time: 0.051, memory: 2944, loss_rpn_cls: 0.1795, loss_rpn_bbox: 0.1435, loss_cls: 0.2954, acc: 87.4375, loss_bbox: 0.2068, loss: 0.8251\n",
      "2022-04-25 09:42:05,960 - mmdet - INFO - Epoch [3][100/414]\tlr: 2.000e-02, eta: 0:03:38, time: 0.300, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.1825, loss_rpn_bbox: 0.1391, loss_cls: 0.2880, acc: 87.8809, loss_bbox: 0.1976, loss: 0.8072\n",
      "2022-04-25 09:42:21,146 - mmdet - INFO - Epoch [3][150/414]\tlr: 2.000e-02, eta: 0:03:23, time: 0.304, data_time: 0.009, memory: 2944, loss_rpn_cls: 0.1622, loss_rpn_bbox: 0.1438, loss_cls: 0.2911, acc: 87.5332, loss_bbox: 0.2151, loss: 0.8122\n",
      "2022-04-25 09:42:36,395 - mmdet - INFO - Epoch [3][200/414]\tlr: 2.000e-02, eta: 0:03:08, time: 0.305, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1700, loss_rpn_bbox: 0.1375, loss_cls: 0.2941, acc: 87.3457, loss_bbox: 0.2153, loss: 0.8169\n",
      "2022-04-25 09:42:51,465 - mmdet - INFO - Epoch [3][250/414]\tlr: 2.000e-02, eta: 0:02:53, time: 0.301, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.1681, loss_rpn_bbox: 0.1385, loss_cls: 0.2837, acc: 87.9414, loss_bbox: 0.2156, loss: 0.8059\n",
      "2022-04-25 09:43:06,533 - mmdet - INFO - Epoch [3][300/414]\tlr: 2.000e-02, eta: 0:02:38, time: 0.301, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.1661, loss_rpn_bbox: 0.1422, loss_cls: 0.2844, acc: 87.8633, loss_bbox: 0.2141, loss: 0.8068\n",
      "2022-04-25 09:43:21,550 - mmdet - INFO - Epoch [3][350/414]\tlr: 2.000e-02, eta: 0:02:23, time: 0.300, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1517, loss_rpn_bbox: 0.1415, loss_cls: 0.2804, acc: 88.2266, loss_bbox: 0.2140, loss: 0.7877\n",
      "2022-04-25 09:43:36,550 - mmdet - INFO - Epoch [3][400/414]\tlr: 2.000e-02, eta: 0:02:08, time: 0.300, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.1494, loss_rpn_bbox: 0.1308, loss_cls: 0.2812, acc: 87.8223, loss_bbox: 0.2089, loss: 0.7703\n",
      "2022-04-25 09:43:40,594 - mmdet - INFO - Saving checkpoint at 3 epochs\n",
      "2022-04-25 09:43:59,123 - mmdet - INFO - Epoch [4][50/414]\tlr: 2.000e-03, eta: 0:01:48, time: 0.346, data_time: 0.052, memory: 2944, loss_rpn_cls: 0.1414, loss_rpn_bbox: 0.1219, loss_cls: 0.2542, acc: 89.1719, loss_bbox: 0.2076, loss: 0.7251\n",
      "2022-04-25 09:44:14,401 - mmdet - INFO - Epoch [4][100/414]\tlr: 2.000e-03, eta: 0:01:33, time: 0.305, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1199, loss_rpn_bbox: 0.1132, loss_cls: 0.2600, acc: 88.9160, loss_bbox: 0.2110, loss: 0.7040\n",
      "2022-04-25 09:44:29,838 - mmdet - INFO - Epoch [4][150/414]\tlr: 2.000e-03, eta: 0:01:19, time: 0.309, data_time: 0.011, memory: 2944, loss_rpn_cls: 0.1209, loss_rpn_bbox: 0.1156, loss_cls: 0.2532, acc: 89.2461, loss_bbox: 0.2145, loss: 0.7042\n",
      "2022-04-25 09:44:45,101 - mmdet - INFO - Epoch [4][200/414]\tlr: 2.000e-03, eta: 0:01:04, time: 0.305, data_time: 0.009, memory: 2944, loss_rpn_cls: 0.1223, loss_rpn_bbox: 0.1072, loss_cls: 0.2415, acc: 89.7891, loss_bbox: 0.2064, loss: 0.6774\n",
      "2022-04-25 09:45:00,388 - mmdet - INFO - Epoch [4][250/414]\tlr: 2.000e-03, eta: 0:00:49, time: 0.306, data_time: 0.009, memory: 2944, loss_rpn_cls: 0.1129, loss_rpn_bbox: 0.1036, loss_cls: 0.2463, acc: 89.6348, loss_bbox: 0.2070, loss: 0.6698\n",
      "2022-04-25 09:45:15,344 - mmdet - INFO - Epoch [4][300/414]\tlr: 2.000e-03, eta: 0:00:34, time: 0.299, data_time: 0.007, memory: 2944, loss_rpn_cls: 0.1165, loss_rpn_bbox: 0.1072, loss_cls: 0.2430, acc: 89.7129, loss_bbox: 0.2114, loss: 0.6781\n",
      "2022-04-25 09:45:30,493 - mmdet - INFO - Epoch [4][350/414]\tlr: 2.000e-03, eta: 0:00:19, time: 0.303, data_time: 0.008, memory: 2944, loss_rpn_cls: 0.1090, loss_rpn_bbox: 0.1054, loss_cls: 0.2409, acc: 89.7891, loss_bbox: 0.2067, loss: 0.6620\n",
      "2022-04-25 09:45:45,464 - mmdet - INFO - Epoch [4][400/414]\tlr: 2.000e-03, eta: 0:00:04, time: 0.300, data_time: 0.009, memory: 2944, loss_rpn_cls: 0.1172, loss_rpn_bbox: 0.1053, loss_cls: 0.2457, acc: 89.6973, loss_bbox: 0.2034, loss: 0.6717\n",
      "2022-04-25 09:45:49,501 - mmdet - INFO - Saving checkpoint at 4 epochs\n"
     ]
    }
   ],
   "source": [
    "train_model(model, datasets, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5232ef9-878b-4e48-819c-c1d832186369",
   "metadata": {},
   "source": [
    "### 配置 DeepSORT 中的 ReID 重识别算法模块\n",
    "\n",
    "ReID 重识别算法本质上是一个多分类图像分类模型，输入行人的 patch 图，提取图中特征，输出 ID 号。\n",
    "\n",
    "用来从图像特征中判断当前行人是否是新人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cf45e1b-e09d-44c6-ad7b-41fcbfbd4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "cfg = mmcv.Config.fromfile('./configs/reid/resnet50_b32x8_MOT17.py')\n",
    "cfg.data_root = 'data/MOT17_tiny/'\n",
    "cfg.data.test.ann_file = cfg.data.test.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.ann_file = 'data/MOT17_tiny/reid/meta/train_9.txt'\n",
    "cfg.data.val.ann_file = cfg.data.val.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "cfg.data.test.data_prefix = cfg.data.test.data_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.data_prefix = cfg.data.train.data_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.val.data_prefix = cfg.data.val.data_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "# 学习率策略\n",
    "cfg.lr_config = dict(\n",
    "    policy='step',\n",
    "    warmup='linear',\n",
    "    warmup_iters=200,\n",
    "    warmup_ratio=1.0 / 200,\n",
    "    step=[1])\n",
    "\n",
    "cfg.total_epochs = 2\n",
    "\n",
    "cfg.work_dir = './tutorial_exps/reid'\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f9eede5-8f5d-46f8-9ad9-b9738bb71266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_type = 'ReIDDataset'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadMultiImagesFromFile', to_float32=True),\n",
      "    dict(\n",
      "        type='SeqResize',\n",
      "        img_scale=(128, 256),\n",
      "        share_params=False,\n",
      "        keep_ratio=False,\n",
      "        bbox_clip_border=False,\n",
      "        override=False),\n",
      "    dict(\n",
      "        type='SeqRandomFlip',\n",
      "        share_params=False,\n",
      "        flip_ratio=0.5,\n",
      "        direction='horizontal'),\n",
      "    dict(\n",
      "        type='SeqNormalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='VideoCollect', keys=['img', 'gt_label']),\n",
      "    dict(type='ReIDFormatBundle')\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='Resize', img_scale=(128, 256), keep_ratio=False),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='ImageToTensor', keys=['img']),\n",
      "    dict(type='Collect', keys=['img'], meta_keys=[])\n",
      "]\n",
      "data_root = 'data/MOT17_tiny/'\n",
      "data = dict(\n",
      "    samples_per_gpu=1,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='ReIDDataset',\n",
      "        triplet_sampler=dict(num_ids=8, ins_per_id=4),\n",
      "        data_prefix='data/MOT17_tiny/reid/imgs',\n",
      "        ann_file='data/MOT17_tiny/reid/meta/train_9.txt',\n",
      "        pipeline=[\n",
      "            dict(type='LoadMultiImagesFromFile', to_float32=True),\n",
      "            dict(\n",
      "                type='SeqResize',\n",
      "                img_scale=(128, 256),\n",
      "                share_params=False,\n",
      "                keep_ratio=False,\n",
      "                bbox_clip_border=False,\n",
      "                override=False),\n",
      "            dict(\n",
      "                type='SeqRandomFlip',\n",
      "                share_params=False,\n",
      "                flip_ratio=0.5,\n",
      "                direction='horizontal'),\n",
      "            dict(\n",
      "                type='SeqNormalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='VideoCollect', keys=['img', 'gt_label']),\n",
      "            dict(type='ReIDFormatBundle')\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='ReIDDataset',\n",
      "        triplet_sampler=None,\n",
      "        data_prefix='data/MOT17_tiny/reid/imgs',\n",
      "        ann_file='data/MOT17_tiny/reid/meta/val_20.txt',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='Resize', img_scale=(128, 256), keep_ratio=False),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'], meta_keys=[])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='ReIDDataset',\n",
      "        triplet_sampler=None,\n",
      "        data_prefix='data/MOT17_tiny/reid/imgs',\n",
      "        ann_file='data/MOT17_tiny/reid/meta/val_20.txt',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='Resize', img_scale=(128, 256), keep_ratio=False),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'], meta_keys=[])\n",
      "        ]))\n",
      "evaluation = dict(interval=1, metric='mAP')\n",
      "optimizer = dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "TRAIN_REID = True\n",
      "model = dict(\n",
      "    reid=dict(\n",
      "        type='BaseReID',\n",
      "        backbone=dict(\n",
      "            type='ResNet',\n",
      "            depth=50,\n",
      "            num_stages=4,\n",
      "            out_indices=(3, ),\n",
      "            style='pytorch'),\n",
      "        neck=dict(type='GlobalAveragePooling', kernel_size=(8, 4), stride=1),\n",
      "        head=dict(\n",
      "            type='LinearReIDHead',\n",
      "            num_fcs=1,\n",
      "            in_channels=2048,\n",
      "            fc_channels=1024,\n",
      "            out_channels=128,\n",
      "            num_classes=380,\n",
      "            loss=dict(type='CrossEntropyLoss', loss_weight=1.0),\n",
      "            loss_pairwise=dict(\n",
      "                type='TripletLoss', margin=0.3, loss_weight=1.0),\n",
      "            norm_cfg=dict(type='BN1d'),\n",
      "            act_cfg=dict(type='ReLU')),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint=\n",
      "            'https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_batch256_imagenet_20200708-cfb998bf.pth'\n",
      "        )))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=200,\n",
      "    warmup_ratio=0.005,\n",
      "    step=[1])\n",
      "total_epochs = 2\n",
      "work_dir = './tutorial_exps/reid'\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a268637-fa50-4b99-a531-cdaa9054f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 09:52:01,538 - mmcv - INFO - initialize BaseReID with init_cfg {'type': 'Pretrained', 'checkpoint': 'https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_batch256_imagenet_20200708-cfb998bf.pth'}\n",
      "2022-04-25 09:52:01,540 - mmcv - INFO - load model from: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_batch256_imagenet_20200708-cfb998bf.pth\n",
      "2022-04-25 09:52:01,541 - mmcv - INFO - load checkpoint from http path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_batch256_imagenet_20200708-cfb998bf.pth\n",
      "Downloading: \"https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_batch256_imagenet_20200708-cfb998bf.pth\" to /home/featurize/.cache/torch/hub/checkpoints/resnet50_batch256_imagenet_20200708-cfb998bf.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0524e52c8b4670ad2b63b786e1c9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 09:52:03,215 - mmcv - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: head.fc.weight, head.fc.bias\n",
      "\n",
      "missing keys in source state_dict: head.fcs.0.fc.weight, head.fcs.0.fc.bias, head.fcs.0.bn.weight, head.fcs.0.bn.bias, head.fcs.0.bn.running_mean, head.fcs.0.bn.running_var, head.fc_out.weight, head.fc_out.bias, head.bn.weight, head.bn.bias, head.bn.running_mean, head.bn.running_var, head.classifier.weight, head.classifier.bias\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmtrack.datasets import build_dataset\n",
    "from mmdet.apis import train_detector as train_model\n",
    "from mmtrack.models import build_reid as build_model\n",
    "\n",
    "\n",
    "model = build_model(cfg.model.reid)\n",
    "model.init_weights()\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "model.CLASSES = datasets[0].CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1890401-eead-4f74-9cc2-e6105342d41d",
   "metadata": {},
   "source": [
    "### 训练 DeepSORT 中的 ReID 重识别算法模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "782f64d1-ae99-46b7-89c7-21a6e06f76d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 09:52:06,400 - mmdet - INFO - Start running, host: featurize@featurize, work_dir: /home/featurize/work/MMTracking教程/0421/mmtracking/tutorial_exps/reid\n",
      "2022-04-25 09:52:06,402 - mmdet - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-04-25 09:52:06,403 - mmdet - INFO - workflow: [('train', 1)], max: 2 epochs\n",
      "2022-04-25 09:52:06,404 - mmdet - INFO - Checkpoints will be saved to /home/featurize/work/MMTracking教程/0421/mmtracking/tutorial_exps/reid by HardDiskBackend.\n",
      "2022-04-25 09:52:22,658 - mmdet - INFO - Epoch [1][50/1576]\tlr: 2.488e-02, eta: 0:16:45, time: 0.324, data_time: 0.185, memory: 2944, triplet_loss: 0.3868, ce_loss: 1.0064, top-1: 88.1875, loss: 1.3932\n",
      "2022-04-25 09:52:33,166 - mmdet - INFO - Epoch [1][100/1576]\tlr: 4.975e-02, eta: 0:13:35, time: 0.210, data_time: 0.066, memory: 2944, triplet_loss: 0.1275, ce_loss: 0.0549, top-1: 98.8750, loss: 0.1824\n",
      "2022-04-25 09:52:40,871 - mmdet - INFO - Epoch [1][150/1576]\tlr: 7.463e-02, eta: 0:11:28, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0278, ce_loss: 0.0401, top-1: 98.9375, loss: 0.0678\n",
      "2022-04-25 09:52:48,449 - mmdet - INFO - Epoch [1][200/1576]\tlr: 9.950e-02, eta: 0:10:19, time: 0.152, data_time: 0.006, memory: 2944, triplet_loss: 0.1508, ce_loss: 0.0408, top-1: 99.0000, loss: 0.1917\n",
      "2022-04-25 09:52:56,020 - mmdet - INFO - Epoch [1][250/1576]\tlr: 1.000e-01, eta: 0:09:35, time: 0.151, data_time: 0.005, memory: 2944, triplet_loss: 0.9537, ce_loss: 0.4394, top-1: 89.3125, loss: 1.3932\n",
      "2022-04-25 09:53:03,615 - mmdet - INFO - Epoch [1][300/1576]\tlr: 1.000e-01, eta: 0:09:03, time: 0.152, data_time: 0.007, memory: 2944, triplet_loss: 0.3158, ce_loss: 0.0859, top-1: 98.6875, loss: 0.4017\n",
      "2022-04-25 09:53:17,378 - mmdet - INFO - Epoch [1][350/1576]\tlr: 1.000e-01, eta: 0:09:27, time: 0.275, data_time: 0.140, memory: 2944, triplet_loss: 0.0124, ce_loss: 0.0125, top-1: 99.4375, loss: 0.0249\n",
      "2022-04-25 09:53:30,388 - mmdet - INFO - Epoch [1][400/1576]\tlr: 1.000e-01, eta: 0:09:37, time: 0.260, data_time: 0.127, memory: 2944, triplet_loss: 0.0006, ce_loss: 0.0010, top-1: 99.9375, loss: 0.0016\n",
      "2022-04-25 09:53:41,429 - mmdet - INFO - Epoch [1][450/1576]\tlr: 1.000e-01, eta: 0:09:30, time: 0.221, data_time: 0.084, memory: 2944, triplet_loss: 0.0056, ce_loss: 0.0003, top-1: 100.0000, loss: 0.0059\n",
      "2022-04-25 09:53:49,204 - mmdet - INFO - Epoch [1][500/1576]\tlr: 1.000e-01, eta: 0:09:04, time: 0.155, data_time: 0.005, memory: 2944, triplet_loss: 0.0017, ce_loss: 0.0004, top-1: 100.0000, loss: 0.0020\n",
      "2022-04-25 09:53:59,050 - mmdet - INFO - Epoch [1][550/1576]\tlr: 1.000e-01, eta: 0:08:52, time: 0.197, data_time: 0.051, memory: 2944, triplet_loss: 0.0043, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0044\n",
      "2022-04-25 09:54:06,857 - mmdet - INFO - Epoch [1][600/1576]\tlr: 1.000e-01, eta: 0:08:32, time: 0.156, data_time: 0.005, memory: 2944, triplet_loss: 0.0013, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0014\n",
      "2022-04-25 09:54:14,643 - mmdet - INFO - Epoch [1][650/1576]\tlr: 1.000e-01, eta: 0:08:13, time: 0.156, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:54:22,313 - mmdet - INFO - Epoch [1][700/1576]\tlr: 1.000e-01, eta: 0:07:55, time: 0.153, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:54:29,932 - mmdet - INFO - Epoch [1][750/1576]\tlr: 1.000e-01, eta: 0:07:39, time: 0.152, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:54:37,501 - mmdet - INFO - Epoch [1][800/1576]\tlr: 1.000e-01, eta: 0:07:24, time: 0.151, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:54:45,193 - mmdet - INFO - Epoch [1][850/1576]\tlr: 1.000e-01, eta: 0:07:09, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:54:52,843 - mmdet - INFO - Epoch [1][900/1576]\tlr: 1.000e-01, eta: 0:06:56, time: 0.153, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:55:01,598 - mmdet - INFO - Epoch [1][950/1576]\tlr: 1.000e-01, eta: 0:06:45, time: 0.175, data_time: 0.024, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:55:09,429 - mmdet - INFO - Epoch [1][1000/1576]\tlr: 1.000e-01, eta: 0:06:33, time: 0.157, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:55:17,125 - mmdet - INFO - Epoch [1][1050/1576]\tlr: 1.000e-01, eta: 0:06:21, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:55:24,912 - mmdet - INFO - Epoch [1][1100/1576]\tlr: 1.000e-01, eta: 0:06:10, time: 0.156, data_time: 0.006, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:55:32,620 - mmdet - INFO - Epoch [1][1150/1576]\tlr: 1.000e-01, eta: 0:05:58, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:55:40,298 - mmdet - INFO - Epoch [1][1200/1576]\tlr: 1.000e-01, eta: 0:05:47, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:55:47,957 - mmdet - INFO - Epoch [1][1250/1576]\tlr: 1.000e-01, eta: 0:05:37, time: 0.153, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:55:58,800 - mmdet - INFO - Epoch [1][1300/1576]\tlr: 1.000e-01, eta: 0:05:30, time: 0.217, data_time: 0.083, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:56:06,626 - mmdet - INFO - Epoch [1][1350/1576]\tlr: 1.000e-01, eta: 0:05:20, time: 0.157, data_time: 0.006, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:56:14,448 - mmdet - INFO - Epoch [1][1400/1576]\tlr: 1.000e-01, eta: 0:05:10, time: 0.156, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:56:22,268 - mmdet - INFO - Epoch [1][1450/1576]\tlr: 1.000e-01, eta: 0:05:00, time: 0.156, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:56:30,063 - mmdet - INFO - Epoch [1][1500/1576]\tlr: 1.000e-01, eta: 0:04:50, time: 0.156, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:56:40,112 - mmdet - INFO - Epoch [1][1550/1576]\tlr: 1.000e-01, eta: 0:04:42, time: 0.201, data_time: 0.055, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:56:44,180 - mmdet - INFO - Saving checkpoint at 1 epochs\n",
      "2022-04-25 09:56:59,364 - mmdet - INFO - Epoch [2][50/1576]\tlr: 1.000e-02, eta: 0:04:26, time: 0.208, data_time: 0.058, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:57:07,166 - mmdet - INFO - Epoch [2][100/1576]\tlr: 1.000e-02, eta: 0:04:17, time: 0.156, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:57:17,250 - mmdet - INFO - Epoch [2][150/1576]\tlr: 1.000e-02, eta: 0:04:09, time: 0.202, data_time: 0.057, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:57:24,997 - mmdet - INFO - Epoch [2][200/1576]\tlr: 1.000e-02, eta: 0:03:59, time: 0.155, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:57:32,695 - mmdet - INFO - Epoch [2][250/1576]\tlr: 1.000e-02, eta: 0:03:50, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:57:40,989 - mmdet - INFO - Epoch [2][300/1576]\tlr: 1.000e-02, eta: 0:03:41, time: 0.166, data_time: 0.022, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:57:48,676 - mmdet - INFO - Epoch [2][350/1576]\tlr: 1.000e-02, eta: 0:03:32, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:57:56,228 - mmdet - INFO - Epoch [2][400/1576]\tlr: 1.000e-02, eta: 0:03:22, time: 0.151, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0000, top-1: 100.0000, loss: 0.0000\n",
      "2022-04-25 09:58:04,187 - mmdet - INFO - Epoch [2][450/1576]\tlr: 1.000e-02, eta: 0:03:13, time: 0.159, data_time: 0.006, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:58:12,799 - mmdet - INFO - Epoch [2][500/1576]\tlr: 1.000e-02, eta: 0:03:05, time: 0.172, data_time: 0.026, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:58:20,580 - mmdet - INFO - Epoch [2][550/1576]\tlr: 1.000e-02, eta: 0:02:56, time: 0.156, data_time: 0.006, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:58:28,330 - mmdet - INFO - Epoch [2][600/1576]\tlr: 1.000e-02, eta: 0:02:47, time: 0.155, data_time: 0.006, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:58:36,145 - mmdet - INFO - Epoch [2][650/1576]\tlr: 1.000e-02, eta: 0:02:38, time: 0.156, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:58:43,959 - mmdet - INFO - Epoch [2][700/1576]\tlr: 1.000e-02, eta: 0:02:29, time: 0.156, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:58:51,775 - mmdet - INFO - Epoch [2][750/1576]\tlr: 1.000e-02, eta: 0:02:20, time: 0.156, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:58:59,561 - mmdet - INFO - Epoch [2][800/1576]\tlr: 1.000e-02, eta: 0:02:12, time: 0.156, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:59:07,280 - mmdet - INFO - Epoch [2][850/1576]\tlr: 1.000e-02, eta: 0:02:03, time: 0.154, data_time: 0.004, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:59:15,532 - mmdet - INFO - Epoch [2][900/1576]\tlr: 1.000e-02, eta: 0:01:54, time: 0.165, data_time: 0.018, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:59:23,563 - mmdet - INFO - Epoch [2][950/1576]\tlr: 1.000e-02, eta: 0:01:46, time: 0.161, data_time: 0.011, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:59:31,286 - mmdet - INFO - Epoch [2][1000/1576]\tlr: 1.000e-02, eta: 0:01:37, time: 0.154, data_time: 0.004, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:59:38,923 - mmdet - INFO - Epoch [2][1050/1576]\tlr: 1.000e-02, eta: 0:01:28, time: 0.153, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:59:46,570 - mmdet - INFO - Epoch [2][1100/1576]\tlr: 1.000e-02, eta: 0:01:20, time: 0.153, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 09:59:54,285 - mmdet - INFO - Epoch [2][1150/1576]\tlr: 1.000e-02, eta: 0:01:11, time: 0.154, data_time: 0.004, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 10:00:02,037 - mmdet - INFO - Epoch [2][1200/1576]\tlr: 1.000e-02, eta: 0:01:03, time: 0.155, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 10:00:09,684 - mmdet - INFO - Epoch [2][1250/1576]\tlr: 1.000e-02, eta: 0:00:54, time: 0.153, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 10:00:17,295 - mmdet - INFO - Epoch [2][1300/1576]\tlr: 1.000e-02, eta: 0:00:46, time: 0.152, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 10:00:25,038 - mmdet - INFO - Epoch [2][1350/1576]\tlr: 1.000e-02, eta: 0:00:37, time: 0.155, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 10:00:32,831 - mmdet - INFO - Epoch [2][1400/1576]\tlr: 1.000e-02, eta: 0:00:29, time: 0.156, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 10:00:40,562 - mmdet - INFO - Epoch [2][1450/1576]\tlr: 1.000e-02, eta: 0:00:21, time: 0.155, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 10:00:49,328 - mmdet - INFO - Epoch [2][1500/1576]\tlr: 1.000e-02, eta: 0:00:12, time: 0.175, data_time: 0.030, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 10:00:57,013 - mmdet - INFO - Epoch [2][1550/1576]\tlr: 1.000e-02, eta: 0:00:04, time: 0.154, data_time: 0.005, memory: 2944, triplet_loss: 0.0000, ce_loss: 0.0001, top-1: 100.0000, loss: 0.0001\n",
      "2022-04-25 10:01:01,098 - mmdet - INFO - Saving checkpoint at 2 epochs\n"
     ]
    }
   ],
   "source": [
    "train_model(model, datasets, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08fb29-da9b-48e7-89d1-eb2b6a67e4b3",
   "metadata": {},
   "source": [
    "## 测试训练得到的 DeepSORT 多目标追踪算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22e5ae-ff59-46d7-bb4a-77db297c61da",
   "metadata": {},
   "source": [
    "### 生成 config 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa417e28-95d7-4777-b196-f42e285b1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "cfg = mmcv.Config.fromfile('./configs/mot/deepsort/deepsort_faster-rcnn_fpn_4e_mot17-private-half.py')\n",
    "cfg.data_root = 'data/MOT17_tiny/'\n",
    "cfg.data.test.ann_file = cfg.data.test.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.ann_file = cfg.data.test.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.val.ann_file = cfg.data.val.ann_file.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "cfg.data.test.img_prefix = cfg.data.test.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.train.img_prefix = cfg.data.train.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "cfg.data.val.img_prefix = cfg.data.val.img_prefix.replace('data/MOT17/','data/MOT17_tiny/')\n",
    "\n",
    "# 指定训练好的目标检测模块和 ReID 重识别模块\n",
    "cfg.model.detector.init_cfg.checkpoint = './tutorial_exps/detector/epoch_4.pth'\n",
    "cfg.model.reid.init_cfg.checkpoint = './tutorial_exps/reid/epoch_2.pth'\n",
    "\n",
    "cfg.work_dir = './tutorial_exps'\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "cfg.data.test.test_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9431013c-d2b6-4d97-9272-0c6914e99ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = dict(\n",
      "    detector=dict(\n",
      "        type='FasterRCNN',\n",
      "        backbone=dict(\n",
      "            type='ResNet',\n",
      "            depth=50,\n",
      "            num_stages=4,\n",
      "            out_indices=(0, 1, 2, 3),\n",
      "            frozen_stages=1,\n",
      "            norm_cfg=dict(type='BN', requires_grad=True),\n",
      "            norm_eval=True,\n",
      "            style='pytorch',\n",
      "            init_cfg=dict(\n",
      "                type='Pretrained', checkpoint='torchvision://resnet50')),\n",
      "        neck=dict(\n",
      "            type='FPN',\n",
      "            in_channels=[256, 512, 1024, 2048],\n",
      "            out_channels=256,\n",
      "            num_outs=5),\n",
      "        rpn_head=dict(\n",
      "            type='RPNHead',\n",
      "            in_channels=256,\n",
      "            feat_channels=256,\n",
      "            anchor_generator=dict(\n",
      "                type='AnchorGenerator',\n",
      "                scales=[8],\n",
      "                ratios=[0.5, 1.0, 2.0],\n",
      "                strides=[4, 8, 16, 32, 64]),\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[1.0, 1.0, 1.0, 1.0],\n",
      "                clip_border=False),\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "            loss_bbox=dict(\n",
      "                type='SmoothL1Loss', beta=0.1111111111111111,\n",
      "                loss_weight=1.0)),\n",
      "        roi_head=dict(\n",
      "            type='StandardRoIHead',\n",
      "            bbox_roi_extractor=dict(\n",
      "                type='SingleRoIExtractor',\n",
      "                roi_layer=dict(\n",
      "                    type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "                out_channels=256,\n",
      "                featmap_strides=[4, 8, 16, 32]),\n",
      "            bbox_head=dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=1,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.1, 0.1, 0.2, 0.2],\n",
      "                    clip_border=False),\n",
      "                reg_class_agnostic=False,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', loss_weight=1.0))),\n",
      "        train_cfg=dict(\n",
      "            rpn=dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.7,\n",
      "                    neg_iou_thr=0.3,\n",
      "                    min_pos_iou=0.3,\n",
      "                    match_low_quality=True,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=256,\n",
      "                    pos_fraction=0.5,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=False),\n",
      "                allowed_border=-1,\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            rpn_proposal=dict(\n",
      "                nms_pre=2000,\n",
      "                max_per_img=1000,\n",
      "                nms=dict(type='nms', iou_threshold=0.7),\n",
      "                min_bbox_size=0),\n",
      "            rcnn=dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    min_pos_iou=0.5,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False)),\n",
      "        test_cfg=dict(\n",
      "            rpn=dict(\n",
      "                nms_pre=1000,\n",
      "                max_per_img=1000,\n",
      "                nms=dict(type='nms', iou_threshold=0.7),\n",
      "                min_bbox_size=0),\n",
      "            rcnn=dict(\n",
      "                score_thr=0.05,\n",
      "                nms=dict(type='nms', iou_threshold=0.5),\n",
      "                max_per_img=100)),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint='./tutorial_exps/detector/epoch_4.pth')),\n",
      "    type='DeepSORT',\n",
      "    motion=dict(type='KalmanFilter', center_only=False),\n",
      "    reid=dict(\n",
      "        type='BaseReID',\n",
      "        backbone=dict(\n",
      "            type='ResNet',\n",
      "            depth=50,\n",
      "            num_stages=4,\n",
      "            out_indices=(3, ),\n",
      "            style='pytorch'),\n",
      "        neck=dict(type='GlobalAveragePooling', kernel_size=(8, 4), stride=1),\n",
      "        head=dict(\n",
      "            type='LinearReIDHead',\n",
      "            num_fcs=1,\n",
      "            in_channels=2048,\n",
      "            fc_channels=1024,\n",
      "            out_channels=128,\n",
      "            num_classes=380,\n",
      "            loss=dict(type='CrossEntropyLoss', loss_weight=1.0),\n",
      "            loss_pairwise=dict(\n",
      "                type='TripletLoss', margin=0.3, loss_weight=1.0),\n",
      "            norm_cfg=dict(type='BN1d'),\n",
      "            act_cfg=dict(type='ReLU')),\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained', checkpoint='./tutorial_exps/reid/epoch_2.pth')),\n",
      "    tracker=dict(\n",
      "        type='SortTracker',\n",
      "        obj_score_thr=0.5,\n",
      "        reid=dict(\n",
      "            num_samples=10,\n",
      "            img_scale=(256, 128),\n",
      "            img_norm_cfg=None,\n",
      "            match_score_thr=2.0),\n",
      "        match_iou_thr=0.5,\n",
      "        momentums=None,\n",
      "        num_tentatives=2,\n",
      "        num_frames_retain=100))\n",
      "dataset_type = 'MOTChallengeDataset'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadMultiImagesFromFile', to_float32=True),\n",
      "    dict(type='SeqLoadAnnotations', with_bbox=True, with_track=True),\n",
      "    dict(\n",
      "        type='SeqResize',\n",
      "        img_scale=(1088, 1088),\n",
      "        share_params=True,\n",
      "        ratio_range=(0.8, 1.2),\n",
      "        keep_ratio=True,\n",
      "        bbox_clip_border=False),\n",
      "    dict(type='SeqPhotoMetricDistortion', share_params=True),\n",
      "    dict(\n",
      "        type='SeqRandomCrop',\n",
      "        share_params=False,\n",
      "        crop_size=(1088, 1088),\n",
      "        bbox_clip_border=False),\n",
      "    dict(type='SeqRandomFlip', share_params=True, flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='SeqNormalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='SeqPad', size_divisor=32),\n",
      "    dict(type='MatchInstances', skip_nomatch=True),\n",
      "    dict(\n",
      "        type='VideoCollect',\n",
      "        keys=[\n",
      "            'img', 'gt_bboxes', 'gt_labels', 'gt_match_indices',\n",
      "            'gt_instance_ids'\n",
      "        ]),\n",
      "    dict(type='SeqDefaultFormatBundle', ref_prefix='ref')\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1088, 1088),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='VideoCollect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data_root = 'data/MOT17_tiny/'\n",
      "data = dict(\n",
      "    samples_per_gpu=2,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='MOTChallengeDataset',\n",
      "        visibility_thr=-1,\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        ref_img_sampler=dict(\n",
      "            num_ref_imgs=1,\n",
      "            frame_range=10,\n",
      "            filter_key_img=True,\n",
      "            method='uniform'),\n",
      "        pipeline=[\n",
      "            dict(type='LoadMultiImagesFromFile', to_float32=True),\n",
      "            dict(type='SeqLoadAnnotations', with_bbox=True, with_track=True),\n",
      "            dict(\n",
      "                type='SeqResize',\n",
      "                img_scale=(1088, 1088),\n",
      "                share_params=True,\n",
      "                ratio_range=(0.8, 1.2),\n",
      "                keep_ratio=True,\n",
      "                bbox_clip_border=False),\n",
      "            dict(type='SeqPhotoMetricDistortion', share_params=True),\n",
      "            dict(\n",
      "                type='SeqRandomCrop',\n",
      "                share_params=False,\n",
      "                crop_size=(1088, 1088),\n",
      "                bbox_clip_border=False),\n",
      "            dict(type='SeqRandomFlip', share_params=True, flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='SeqNormalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='SeqPad', size_divisor=32),\n",
      "            dict(type='MatchInstances', skip_nomatch=True),\n",
      "            dict(\n",
      "                type='VideoCollect',\n",
      "                keys=[\n",
      "                    'img', 'gt_bboxes', 'gt_labels', 'gt_match_indices',\n",
      "                    'gt_instance_ids'\n",
      "                ]),\n",
      "            dict(type='SeqDefaultFormatBundle', ref_prefix='ref')\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='MOTChallengeDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        ref_img_sampler=None,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1088, 1088),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='VideoCollect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='MOTChallengeDataset',\n",
      "        ann_file='data/MOT17_tiny/annotations/half-val_cocoformat.json',\n",
      "        img_prefix='data/MOT17_tiny/train',\n",
      "        ref_img_sampler=None,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1088, 1088),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='VideoCollect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        test_mode=True))\n",
      "optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=100,\n",
      "    warmup_ratio=0.01,\n",
      "    step=[3])\n",
      "total_epochs = 4\n",
      "evaluation = dict(metric=['bbox', 'track'], interval=1)\n",
      "search_metrics = ['MOTA', 'IDF1', 'FN', 'FP', 'IDs', 'MT', 'ML']\n",
      "work_dir = './tutorial_exps'\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710146c3-100e-46e4-848d-718ade78e3bc",
   "metadata": {},
   "source": [
    "### 保存 config 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee26ae-e3d4-489c-987e-2f5f8e65dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://zihao-openmmlab.obs.cn-east-3.myhuaweicloud.com/20220418-mmtracking/deepsort_faster-rcnn_fpn_4e_mot17-new.py -O configs/mot/deepsort/deepsort_faster-rcnn_fpn_4e_mot17-new.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3318d6f9-d8c0-440c-a1c4-ea9c4039a64c",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23489871-1fe6-42a5-9ce4-43b01c31fda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 10:08:12,534 - mmcv - INFO - initialize FasterRCNN with init_cfg {'type': 'Pretrained', 'checkpoint': './tutorial_exps/detector/epoch_4.pth'}\n",
      "2022-04-25 10:08:12,536 - mmcv - INFO - load model from: ./tutorial_exps/detector/epoch_4.pth\n",
      "2022-04-25 10:08:12,536 - mmcv - INFO - load checkpoint from local path: ./tutorial_exps/detector/epoch_4.pth\n",
      "2022-04-25 10:08:12,856 - mmcv - INFO - initialize BaseReID with init_cfg {'type': 'Pretrained', 'checkpoint': './tutorial_exps/reid/epoch_2.pth'}\n",
      "2022-04-25 10:08:12,857 - mmcv - INFO - load model from: ./tutorial_exps/reid/epoch_2.pth\n",
      "2022-04-25 10:08:12,858 - mmcv - INFO - load checkpoint from local path: ./tutorial_exps/reid/epoch_2.pth\n",
      "2022-04-25 10:08:13,054 - mmcv - INFO - \n",
      "detector.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,055 - mmcv - INFO - \n",
      "detector.backbone.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,056 - mmcv - INFO - \n",
      "detector.backbone.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,056 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,057 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,058 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,058 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,059 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn2.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,060 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn2.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,061 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,061 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn3.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,062 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn3.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,062 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,063 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.downsample.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,063 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.downsample.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,064 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,064 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,065 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,065 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,066 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn2.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,066 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn2.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,067 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,068 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn3.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,068 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn3.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,068 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,069 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,070 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,070 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,071 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn2.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,071 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn2.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,072 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,072 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn3.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,073 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn3.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:08:13,073 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,074 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,074 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,075 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,076 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,076 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,077 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,077 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,078 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,078 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,079 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.downsample.1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,079 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.downsample.1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,080 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,080 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,081 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,081 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,082 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,082 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,083 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,084 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,084 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,085 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,085 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,086 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,086 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,087 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,087 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,088 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,088 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,089 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,089 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,090 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,090 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,091 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,091 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,092 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,092 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,093 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,093 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,094 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,094 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,100 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,101 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,101 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,102 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,102 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,103 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,103 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,104 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,104 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,105 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,105 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,106 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,111 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,111 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,112 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,112 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,113 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,113 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,114 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,114 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,115 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,115 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,116 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,116 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,117 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,117 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,118 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,120 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,121 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,121 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,122 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,122 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,123 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,123 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,124 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,124 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,125 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,125 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,126 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,126 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,127 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,127 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,128 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,128 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,129 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,129 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,130 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,131 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,133 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,134 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,134 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,135 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,136 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,136 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,137 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,137 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,138 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,138 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,139 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,139 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,140 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,140 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,141 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,141 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,144 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,144 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,145 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,145 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,146 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,146 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,147 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,147 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,148 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,148 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,149 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,149 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,150 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,150 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,151 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,151 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,152 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,152 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,153 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,153 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,154 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,154 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,155 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.0.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,155 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,159 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.1.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,160 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,160 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.2.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,161 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,161 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.3.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,162 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,162 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.0.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,163 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,163 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.1.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,164 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,164 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.2.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,165 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,165 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.3.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,166 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,166 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,167 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_cls.weight - torch.Size([3, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,167 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_cls.bias - torch.Size([3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,168 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_reg.weight - torch.Size([12, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,169 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_reg.bias - torch.Size([12]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,169 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_cls.weight - torch.Size([2, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,170 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_cls.bias - torch.Size([2]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,170 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_reg.weight - torch.Size([4, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,171 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_reg.bias - torch.Size([4]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,171 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,172 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,172 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,173 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:08:13,173 - mmcv - INFO - \n",
      "reid.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,177 - mmcv - INFO - \n",
      "reid.backbone.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,178 - mmcv - INFO - \n",
      "reid.backbone.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,178 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,179 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,180 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,180 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,181 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,181 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,182 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,182 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,183 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,183 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,184 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.downsample.1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,184 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.downsample.1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,185 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,185 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,188 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,188 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,189 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,189 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,190 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,190 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,191 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,191 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,192 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,192 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,193 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,193 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,194 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,194 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,195 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,196 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,196 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,197 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,197 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,198 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,198 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,199 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,199 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,200 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,200 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,204 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,204 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.downsample.1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,205 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.downsample.1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,206 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,206 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,207 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,207 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,208 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,208 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,209 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,209 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,210 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,210 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,211 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,212 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,212 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,213 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,213 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,214 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,214 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,215 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,215 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,216 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,216 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,217 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,217 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,218 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,219 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,222 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,223 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,223 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,224 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,224 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,225 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,225 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,226 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,226 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,227 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,227 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,228 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,229 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,229 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,230 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,230 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,231 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,231 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,232 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,232 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,233 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,233 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,234 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,234 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,235 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,235 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,236 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,236 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,237 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,237 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,238 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,238 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,239 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,239 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,240 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,240 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,241 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,242 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,242 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,243 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,243 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,244 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,244 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,245 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,245 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,246 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,246 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,247 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,247 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,248 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,248 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,249 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,250 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,250 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,252 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,253 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,253 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,254 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,254 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,255 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,255 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,256 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,257 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,257 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,258 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,258 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,259 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,260 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,260 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,261 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,261 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,262 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,262 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,263 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,263 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,264 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,264 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,265 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,265 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,266 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,267 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,267 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,268 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,268 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,269 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,269 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,270 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,270 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,271 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,271 - mmcv - INFO - \n",
      "reid.head.fcs.0.fc.weight - torch.Size([1024, 2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,272 - mmcv - INFO - \n",
      "reid.head.fcs.0.fc.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,273 - mmcv - INFO - \n",
      "reid.head.fcs.0.bn.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,273 - mmcv - INFO - \n",
      "reid.head.fcs.0.bn.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,274 - mmcv - INFO - \n",
      "reid.head.fc_out.weight - torch.Size([128, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,274 - mmcv - INFO - \n",
      "reid.head.fc_out.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,275 - mmcv - INFO - \n",
      "reid.head.bn.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,275 - mmcv - INFO - \n",
      "reid.head.bn.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,276 - mmcv - INFO - \n",
      "reid.head.classifier.weight - torch.Size([380, 128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:08:13,276 - mmcv - INFO - \n",
      "reid.head.classifier.bias - torch.Size([380]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n"
     ]
    }
   ],
   "source": [
    "from mmtrack.datasets import build_dataloader\n",
    "from mmtrack.models import build_model\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmtrack.apis import single_gpu_test\n",
    "from mmtrack.datasets import build_dataset\n",
    "\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=1,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=False,\n",
    "    shuffle=False)\n",
    "\n",
    "# 构建模型，载入 checkpoint 权重文件\n",
    "model = build_model(cfg.model)\n",
    "model.init_weights()\n",
    "\n",
    "model = MMDataParallel(model, device_ids=cfg.gpu_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e8883-0fb7-444a-8f92-75f365ed73cb",
   "metadata": {},
   "source": [
    "### 在测试集上预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9e01cea-53fb-43c0-9096-7089cc5a7e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 823/823, 4.8 task/s, elapsed: 170s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "outputs = single_gpu_test(model, data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caed3481-40f7-4d27-8f27-1ad5e109c0c4",
   "metadata": {},
   "source": [
    "### 在测试集上评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a50ecdc-f362-4223-9fb4-91b94e3cca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate CLEAR MOT results.\n",
      "\n",
      "Eval Config:\n",
      "USE_PARALLEL         : False                         \n",
      "NUM_PARALLEL_CORES   : 8                             \n",
      "BREAK_ON_ERROR       : True                          \n",
      "RETURN_ON_ERROR      : False                         \n",
      "LOG_ON_ERROR         : /environment/miniconda3/lib/python3.7/site-packages/error_log.txt\n",
      "PRINT_RESULTS        : True                          \n",
      "PRINT_ONLY_COMBINED  : False                         \n",
      "PRINT_CONFIG         : True                          \n",
      "TIME_PROGRESS        : True                          \n",
      "DISPLAY_LESS_PROGRESS : True                          \n",
      "OUTPUT_SUMMARY       : True                          \n",
      "OUTPUT_EMPTY_CLASSES : True                          \n",
      "OUTPUT_DETAILED      : True                          \n",
      "PLOT_CURVES          : True                          \n",
      "\n",
      "MotChallenge2DBox Config:\n",
      "GT_FOLDER            : data/MOT17_tiny/train         \n",
      "TRACKERS_FOLDER      : /tmp/tmpxnoiudbt              \n",
      "OUTPUT_FOLDER        : None                          \n",
      "TRACKERS_TO_EVAL     : ['track']                     \n",
      "CLASSES_TO_EVAL      : ['pedestrian']                \n",
      "BENCHMARK            : MOT17                         \n",
      "SPLIT_TO_EVAL        : train                         \n",
      "INPUT_AS_ZIP         : False                         \n",
      "PRINT_CONFIG         : True                          \n",
      "DO_PREPROC           : True                          \n",
      "TRACKER_SUB_FOLDER   :                               \n",
      "OUTPUT_SUB_FOLDER    :                               \n",
      "TRACKER_DISPLAY_NAMES : None                          \n",
      "SEQMAP_FOLDER        : None                          \n",
      "SEQMAP_FILE          : /tmp/tmpxnoiudbt/videoseq.txt \n",
      "SEQ_INFO             : None                          \n",
      "GT_LOC_FORMAT        : {gt_folder}/{seq}/gt/gt_half-val.txt\n",
      "SKIP_SPLIT_FOL       : True                          \n",
      "\n",
      "Evaluating 1 tracker(s) on 2 sequence(s) for 1 class(es) on MotChallenge2DBox dataset using the following metrics: HOTA, Count\n",
      "\n",
      "\n",
      "Evaluating track\n",
      "\n",
      "1 eval_sequence(MOT17-02-FRCNN, track)                                   0.7235 sec\n",
      "2 eval_sequence(MOT17-04-FRCNN, track)                                   1.2668 sec\n",
      "\n",
      "All sequences for track finished in 1.99 seconds\n",
      "\n",
      "HOTA: track-pedestrian             HOTA      DetA      AssA      DetRe     DetPr     AssRe     AssPr     LocA      RHOTA     HOTA(0)   LocA(0)   HOTALocA(0)\n",
      "MOT17-02-FRCNN                     16.088    24.38     11.289    34.442    38.472    11.957    66.846    72.026    19.306    24.815    53.99     13.398    \n",
      "MOT17-04-FRCNN                     29.585    43.034    21.34     55.405    57.013    23.123    63.882    77.994    33.845    40.237    68.305    27.484    \n",
      "COMBINED                           26.154    37.165    19.553    49.324    51.942    21.218    64.781    76.723    30.407    36.233    64.797    23.478    \n",
      "\n",
      "Count: track-pedestrian            Dets      GT_Dets   IDs       GT_IDs    \n",
      "MOT17-02-FRCNN                     8845      9880      1395      53        \n",
      "MOT17-04-FRCNN                     23496     24178     1144      69        \n",
      "COMBINED                           32341     34058     2539      122       \n",
      "                IDF1   IDP   IDR  Rcll  Prcn  GT MT PT ML    FP    FN  IDs   FM   MOTA  MOTP IDt IDa IDm      HOTA\n",
      "MOT17-02-FRCNN 17.3% 18.3% 16.4% 41.9% 46.8%  53  6 27 20  4707  5742  539  414 -11.2% 0.306  56 436   2  0.160876\n",
      "MOT17-04-FRCNN 29.4% 29.9% 29.0% 67.9% 69.9%  69 25 39  5  7077  7759 1283  581  33.3% 0.246 304 517   4  0.295849\n",
      "OVERALL        26.0% 26.7% 25.4% 60.4% 63.6% 122 31 66 25 11784 13501 1822  995  20.4% 0.258 360 953   6  0.261538\n",
      "{'IDF1': 0.26, 'IDP': 0.267, 'IDR': 0.254, 'Rcll': 0.604, 'Prcn': 0.636, 'GT': 122, 'MT': 31, 'PT': 66, 'ML': 25, 'FP': 11784, 'FN': 13501, 'IDs': 1822, 'FM': 995, 'MOTA': 0.204, 'MOTP': 0.258, 'IDt': 360, 'IDa': 953, 'IDm': 6, 'HOTA': 0.262}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_kwargs = cfg.get('evaluation', {}).copy()\n",
    "# hard-code way to remove EvalHook args\n",
    "eval_hook_args = [\n",
    "    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',\n",
    "    'rule', 'by_epoch'\n",
    "]\n",
    "for key in eval_hook_args:\n",
    "    eval_kwargs.pop(key, None)\n",
    "eval_kwargs.update(dict(metric=['track']))\n",
    "metric = dataset.evaluate(outputs, **eval_kwargs)\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a7c3e5-e060-4ebc-bcc2-fbdc9693cb97",
   "metadata": {},
   "source": [
    "### 对新视频预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f2d30bc-a152-4310-9e1f-7b93112cbe2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-25 10:34:31,364 - mmtrack - INFO - initialize FasterRCNN with init_cfg {'type': 'Pretrained', 'checkpoint': './tutorial_exps/detector/epoch_4.pth'}\n",
      "2022-04-25 10:34:31,365 - mmcv - INFO - load model from: ./tutorial_exps/detector/epoch_4.pth\n",
      "2022-04-25 10:34:31,365 - mmcv - INFO - load checkpoint from local path: ./tutorial_exps/detector/epoch_4.pth\n",
      "2022-04-25 10:34:34,905 - mmtrack - INFO - initialize BaseReID with init_cfg {'type': 'Pretrained', 'checkpoint': './tutorial_exps/reid/epoch_2.pth'}\n",
      "2022-04-25 10:34:34,905 - mmcv - INFO - load model from: ./tutorial_exps/reid/epoch_2.pth\n",
      "2022-04-25 10:34:34,905 - mmcv - INFO - load checkpoint from local path: ./tutorial_exps/reid/epoch_2.pth\n",
      "Warning: The model doesn't have classes\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 99/99, 4.9 task/s, elapsed: 20s, ETA:     0smaking the output video at outputs/I1_MOT_people_short.mp4 with a FPS of 24\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 99/99, 26.6 task/s, elapsed: 4s, ETA:     0s\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 命令行方式实现\n",
    "# Deepsort算法仅需指定 config 文件，不需指定 checkpoint 文件\n",
    "!python demo/demo_mot_vis.py \\\n",
    "        configs/mot/deepsort/deepsort_faster-rcnn_fpn_4e_mot17-new.py \\\n",
    "        --input data/mot_people_short.mp4 \\\n",
    "        --output outputs/I1_MOT_people_short.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a934a9b1-838d-4c20-be58-7f9161735cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 10:35:02,141 - mmcv - INFO - initialize FasterRCNN with init_cfg {'type': 'Pretrained', 'checkpoint': './tutorial_exps/detector/epoch_4.pth'}\n",
      "2022-04-25 10:35:02,142 - mmcv - INFO - load model from: ./tutorial_exps/detector/epoch_4.pth\n",
      "2022-04-25 10:35:02,142 - mmcv - INFO - load checkpoint from local path: ./tutorial_exps/detector/epoch_4.pth\n",
      "2022-04-25 10:35:02,399 - mmcv - INFO - initialize BaseReID with init_cfg {'type': 'Pretrained', 'checkpoint': './tutorial_exps/reid/epoch_2.pth'}\n",
      "2022-04-25 10:35:02,400 - mmcv - INFO - load model from: ./tutorial_exps/reid/epoch_2.pth\n",
      "2022-04-25 10:35:02,401 - mmcv - INFO - load checkpoint from local path: ./tutorial_exps/reid/epoch_2.pth\n",
      "2022-04-25 10:35:02,545 - mmcv - INFO - \n",
      "detector.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,546 - mmcv - INFO - \n",
      "detector.backbone.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,547 - mmcv - INFO - \n",
      "detector.backbone.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,548 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,548 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,549 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,549 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,550 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn2.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,550 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn2.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,552 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,552 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn3.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,553 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.bn3.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,554 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,554 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.downsample.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,555 - mmcv - INFO - \n",
      "detector.backbone.layer1.0.downsample.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,555 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,556 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,558 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,558 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,559 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn2.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,560 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn2.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,560 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,561 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn3.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,561 - mmcv - INFO - \n",
      "detector.backbone.layer1.1.bn3.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,562 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,562 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,563 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,565 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,565 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn2.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,566 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn2.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,567 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,568 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn3.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,568 - mmcv - INFO - \n",
      "detector.backbone.layer1.2.bn3.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of DeepSORT  \n",
      " \n",
      "2022-04-25 10:35:02,569 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,569 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,570 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,570 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,571 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,571 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,573 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,573 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,574 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,575 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,576 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.downsample.1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,576 - mmcv - INFO - \n",
      "detector.backbone.layer2.0.downsample.1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,577 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,577 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,578 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,578 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,579 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,579 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,580 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,580 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,581 - mmcv - INFO - \n",
      "detector.backbone.layer2.1.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,582 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,582 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,583 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,583 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,584 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,587 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,587 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,588 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,588 - mmcv - INFO - \n",
      "detector.backbone.layer2.2.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,589 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,589 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,590 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,590 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,591 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,591 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,592 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,592 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,593 - mmcv - INFO - \n",
      "detector.backbone.layer2.3.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,594 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,594 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,597 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,597 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,598 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,598 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,599 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,599 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,600 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,600 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,601 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,601 - mmcv - INFO - \n",
      "detector.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,602 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,602 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,603 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,603 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,604 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,605 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,605 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,606 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,606 - mmcv - INFO - \n",
      "detector.backbone.layer3.1.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,607 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,610 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,610 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,611 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,611 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,612 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,613 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,613 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,614 - mmcv - INFO - \n",
      "detector.backbone.layer3.2.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,614 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,615 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,615 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,616 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,616 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,617 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,617 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,618 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,619 - mmcv - INFO - \n",
      "detector.backbone.layer3.3.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,619 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,620 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,620 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,621 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,621 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,622 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,623 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,623 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,624 - mmcv - INFO - \n",
      "detector.backbone.layer3.4.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,624 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,625 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,625 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,626 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,626 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,630 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,630 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,631 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,633 - mmcv - INFO - \n",
      "detector.backbone.layer3.5.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,634 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,634 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,635 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,635 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,636 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,636 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,637 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,637 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,638 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,639 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,639 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,640 - mmcv - INFO - \n",
      "detector.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,640 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,641 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,641 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,642 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,642 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,643 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,643 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,644 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,644 - mmcv - INFO - \n",
      "detector.backbone.layer4.1.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,645 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,645 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,646 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,646 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,647 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,647 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,648 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,648 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,649 - mmcv - INFO - \n",
      "detector.backbone.layer4.2.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,649 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,650 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.0.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,650 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,651 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.1.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,651 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,652 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.2.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,652 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,653 - mmcv - INFO - \n",
      "detector.neck.lateral_convs.3.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,653 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,654 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.0.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,655 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,655 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.1.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,656 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,656 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.2.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,657 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,657 - mmcv - INFO - \n",
      "detector.neck.fpn_convs.3.conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,664 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,665 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_conv.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,666 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_cls.weight - torch.Size([3, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,666 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_cls.bias - torch.Size([3]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,667 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_reg.weight - torch.Size([12, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,667 - mmcv - INFO - \n",
      "detector.rpn_head.rpn_reg.bias - torch.Size([12]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,668 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_cls.weight - torch.Size([2, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,668 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_cls.bias - torch.Size([2]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,669 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_reg.weight - torch.Size([4, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,669 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.fc_reg.bias - torch.Size([4]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,670 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,670 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,671 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,671 - mmcv - INFO - \n",
      "detector.roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/detector/epoch_4.pth \n",
      " \n",
      "2022-04-25 10:35:02,672 - mmcv - INFO - \n",
      "reid.backbone.conv1.weight - torch.Size([64, 3, 7, 7]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,672 - mmcv - INFO - \n",
      "reid.backbone.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,675 - mmcv - INFO - \n",
      "reid.backbone.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,676 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,676 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,677 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,677 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,678 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,678 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,679 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,680 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,680 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,681 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,681 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.downsample.1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,682 - mmcv - INFO - \n",
      "reid.backbone.layer1.0.downsample.1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,682 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,683 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,684 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,684 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,685 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,685 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,686 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,686 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,687 - mmcv - INFO - \n",
      "reid.backbone.layer1.1.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,687 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,688 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,688 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,689 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,689 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,690 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,690 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,691 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,692 - mmcv - INFO - \n",
      "reid.backbone.layer1.2.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,692 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,693 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,693 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,694 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,694 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,695 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,695 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,696 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,696 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,697 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,697 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.downsample.1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,698 - mmcv - INFO - \n",
      "reid.backbone.layer2.0.downsample.1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,698 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,699 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,700 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,700 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,701 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,701 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,702 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,702 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,703 - mmcv - INFO - \n",
      "reid.backbone.layer2.1.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,703 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,704 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,704 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,705 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,705 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,706 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,706 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,707 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,707 - mmcv - INFO - \n",
      "reid.backbone.layer2.2.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,708 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,708 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,709 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,709 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,710 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,710 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,711 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,711 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,720 - mmcv - INFO - \n",
      "reid.backbone.layer2.3.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,720 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,721 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,721 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,722 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,722 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,722 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,723 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,723 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,724 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,724 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,725 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.downsample.1.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,725 - mmcv - INFO - \n",
      "reid.backbone.layer3.0.downsample.1.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,726 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,726 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,726 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,727 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,727 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,728 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,728 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,729 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,729 - mmcv - INFO - \n",
      "reid.backbone.layer3.1.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,730 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,730 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,730 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,731 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,731 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,732 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,732 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,733 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,733 - mmcv - INFO - \n",
      "reid.backbone.layer3.2.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,734 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,734 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,735 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,735 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,736 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,736 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,737 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,742 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,742 - mmcv - INFO - \n",
      "reid.backbone.layer3.3.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,742 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,743 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,743 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,744 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,745 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,745 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,746 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,746 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,747 - mmcv - INFO - \n",
      "reid.backbone.layer3.4.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,747 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,748 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,748 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,749 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,749 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,750 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,750 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,751 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,751 - mmcv - INFO - \n",
      "reid.backbone.layer3.5.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,752 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,752 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,753 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,753 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,754 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,754 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,754 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,755 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,756 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,756 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,757 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.downsample.1.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,757 - mmcv - INFO - \n",
      "reid.backbone.layer4.0.downsample.1.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,758 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,758 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,759 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,759 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,760 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,760 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,761 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,761 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,762 - mmcv - INFO - \n",
      "reid.backbone.layer4.1.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,762 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,763 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,763 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,764 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,764 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,765 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,765 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,766 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,766 - mmcv - INFO - \n",
      "reid.backbone.layer4.2.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,767 - mmcv - INFO - \n",
      "reid.head.fcs.0.fc.weight - torch.Size([1024, 2048]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,767 - mmcv - INFO - \n",
      "reid.head.fcs.0.fc.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,768 - mmcv - INFO - \n",
      "reid.head.fcs.0.bn.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,768 - mmcv - INFO - \n",
      "reid.head.fcs.0.bn.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,769 - mmcv - INFO - \n",
      "reid.head.fc_out.weight - torch.Size([128, 1024]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,769 - mmcv - INFO - \n",
      "reid.head.fc_out.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,770 - mmcv - INFO - \n",
      "reid.head.bn.weight - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,771 - mmcv - INFO - \n",
      "reid.head.bn.bias - torch.Size([128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,771 - mmcv - INFO - \n",
      "reid.head.classifier.weight - torch.Size([380, 128]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n",
      "2022-04-25 10:35:02,772 - mmcv - INFO - \n",
      "reid.head.classifier.bias - torch.Size([380]): \n",
      "PretrainedInit: load from ./tutorial_exps/reid/epoch_2.pth \n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The model doesn't have classes\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 99/99, 4.7 task/s, elapsed: 21s, ETA:     0s\n",
      " making the output video at outputs/I2_MOT_people_short.mp4 with a FPS of 24.652929091701427\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 99/99, 25.6 task/s, elapsed: 4s, ETA:     0s\n"
     ]
    }
   ],
   "source": [
    "# Python API 方式实现\n",
    "import mmcv\n",
    "import tempfile\n",
    "from mmtrack.apis import inference_mot, init_model\n",
    "\n",
    "# 输入输出视频路径\n",
    "input_video = 'data/mot_people_short.mp4'\n",
    "output = 'outputs/I2_MOT_people_short.mp4'\n",
    "\n",
    "# 指定 config 配置文件 和 模型权重文件，创建模型\n",
    "mot_config = './configs/mot/deepsort/deepsort_faster-rcnn_fpn_4e_mot17-new.py'\n",
    "# 初始化模型\n",
    "mot_model = init_model(mot_config, device='cuda:0')\n",
    "\n",
    "# 读入待预测视频\n",
    "imgs = mmcv.VideoReader(input_video)\n",
    "prog_bar = mmcv.ProgressBar(len(imgs))\n",
    "out_dir = tempfile.TemporaryDirectory()\n",
    "out_path = out_dir.name\n",
    "# 逐帧输入模型预测\n",
    "for i, img in enumerate(imgs):\n",
    "    result = inference_mot(mot_model, img, frame_id=i)\n",
    "    \n",
    "    mot_model.show_result(\n",
    "            img,\n",
    "            result,\n",
    "            show=False,\n",
    "            wait_time=int(1000. / imgs.fps),\n",
    "            out_file=f'{out_path}/{i:06d}.jpg')\n",
    "    prog_bar.update()\n",
    "\n",
    "print(f'\\n making the output video at {output} with a FPS of {imgs.fps}')\n",
    "mmcv.frames2video(out_path, output, fps=imgs.fps, fourcc='mp4v')\n",
    "out_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91c425-2d64-4318-a091-35b0231b3f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
